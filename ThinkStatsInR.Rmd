---
title: "thinkstats"
output: html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(tidyverse)
library(plotly)
```

The data source is obtained from https://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm
This contains fixed width files, and stata dictionaries consisting of columns for the data files
```{r}
#helper function to parse Stata dictionary
dct.parser <- function(dct, includes = c("StartPos", "StorageType", "ColName", 
                                         "ColWidth", "VarLabel"),
                       preview = FALSE) {
  temp <- readLines(dct)
  temp <- temp[grepl("_column", temp)]
  
  if (isTRUE(preview)) {
    head(temp)
  } else {
    possibilities <- c("StartPos", "StorageType", 
                       "ColName", "ColWidth", "VarLabel")
    classes <- c("numeric", "character", "
                 character", "numeric", "character")
    pattern <- c(StartPos = ".*\\(([0-9 ]+)\\)",
                 StorageType = "(byte|int|long|float|double|str[0-9]+)",
                 ColName = "(.*)",
                 ColWidth = "%([0-9.]+)[a-z]+",
                 VarLabel = "(.*)")
    
    mymatch <- match(includes, possibilities)
    
    pattern <- paste(paste(pattern[mymatch], 
                           collapse ="\\s+"), "$", sep = "")    
    
    metadata <- setNames(lapply(seq_along(mymatch), function(x) {
      out <- gsub(pattern, paste("\\", x, sep = ""), temp)
      out <- gsub("^\\s+|\\s+$", "", out)
      out <- gsub('\"', "", out, fixed = TRUE)
      class(out) <- classes[mymatch][x] ; out }), 
                         possibilities[mymatch])
    
    implicit.dec <- grepl("\\.[1-9]", metadata[["ColWidth"]])
    if (any(implicit.dec)) {
      message("Some variables may need to be corrected for implicit decimals. 
              Try 'MESSAGES(output_from_dct.parser)' for more details.")
      metadata[["Decimals"]] <- rep(NA, length(metadata[["ColWidth"]]))
      metadata[["Decimals"]][implicit.dec] <-
        as.numeric(gsub("[0-9]+\\.", "", 
                        metadata[["ColWidth"]][implicit.dec]))
      metadata[["ColWidth"]] <- floor(as.numeric(metadata[["ColWidth"]]))
    }
    
    metadata[["ColName"]] <- make.names(
      gsub("\\s", "", metadata[["ColName"]]))
    
    metadata <- data.frame(metadata)
    
    if ("StorageType" %in% includes) {
      metadata <- 
        within(metadata, {
          colClasses <- ifelse(
            StorageType == "byte", "raw",
            ifelse(StorageType %in% c("double", "long", "float"), 
                   "numeric", 
                   ifelse(StorageType == "int", "integer",
                          ifelse(substr(StorageType, 1, 3) == "str", 
                                 "character", NA))))
        })
    }
    if (any(implicit.dec)) {
      attr(metadata, "MESSAGE") <- c(sprintf("%s", paste(
        "Some variables might need to be corrected for implicit decimals. 
        A variable, 'Decimals', has been created in the metadata that
        indicates the number of decimal places the variable should hold. 
        To correct the output, try (where your stored output is 'mydf'): 
        
        lapply(seq_along(mydf[!is.na(Decimals)]), 
        function(x) mydf[!is.na(Decimals)][x]
        / 10^Decimals[!is.na(Decimals)][x])
        
        The variables in question are:
        ")), sprintf("%s", metadata[["ColName"]][!is.na(metadata[["Decimals"]])]))
            class(attr(metadata, "MESSAGE")) <- c(
                "MESSAGE", class(attr(metadata, "MESSAGE")))
        }
        attr(metadata, "original.dictionary") <- 
            c(dct, basename(dct))
        metadata
    }
}
```


We can read the coulmns from 2002FemPreg.dct and use those columns to import the data from the fixed width file 2002FemPreg.dat
```{r}
femPreg2002columns <- dct.parser('~/Documents/CodeWork/ThinkStats2/code/2002FemPreg.dct')
femPreg2002 <- read.fwf('~/Documents/CodeWork/ThinkStats2/code/2002FemPreg.dat', widths = femPreg2002columns$ColWidth, col.names = femPreg2002columns$ColName)
```

Taking a look at the data
```{r}
head(femPreg2002)
```

We can see a lot of missing values. We'll clean the data for the columns that we want to analyze.

## Transformation

1. agepreg contains the mother's age at the end of the pregnancy. In the data file, agepreg is encoded as an integer number of centiyears. So the first line divides each element of agepreg by 100, yielding a floating-point value in years.

2. birthwgt_lb and birthwgt_oz contain the weight of the baby, in pounds and ounces, for pregnancies that end in live birth. In addition it uses several special codes:
  97 NOT ASCERTAINED
  98 REFUSED
  99 DONT KNOW
Special values encoded as numbers are dangerous because if they are not
handled properly, they can generate bogus results, like a 99-pound baby. Assuming that a baby can't be generally more than 20 lb at birth, we will replace all other values with NA, as they are NOT ASCERTAINED(97),  REFUSED(98), DONT KNOW(99), or invalid values.
Similarly, the age of father has these similar special codes, which we will replace by NA
```{r}
cleanFemPreg <- function(data){
  # mother's age is encoded in centiyears; convert to years
  data['agepreg'] <-  data['agepreg']/100.0
  
  # birthwgt_lb contains at least one bogus value (51 lbs)
  # replace with NaN
  data$birthwgt_lb[data$birthwgt_lb > 20] <- NA
  
  # replace 'not ascertained', 'refused', 'don't know' with NA
  na_vals = c(97, 98, 99)
  data$birthwgt_oz[data$birthwgt_oz %in% na_vals] <- NA
  data$hpagelb[data$hpagelb %in% na_vals] <- NA
  
  # birthweight is stored in two columns, lbs and oz.
  # convert to a single column in lb
  data['totalwgt_lb'] <- data$birthwgt_lb + (data$birthwgt_oz / 16.0)
  
  return (data)
}
```

```{r}
femPregCleaned <- cleanFemPreg(femPreg2002)
```

### Validation
One way to validate data is to compute basic statistics and compare them with published results. For example, the NSFG codebook includes tables that summarize each variable. Here is the table for outcome, which encodes the outcome of each pregnancy:
value     label         Total
1         LIVE BIRTH        9148
2         INDUCED ABORTION  1862
3         STILLBIRTH        120
4         MISCARRIAGE       1921
5         ECTOPIC PREGNANCY 190
6         CURRENT PREGNANCY 352

```{r}
femPreg2002 %>%
  group_by(outcome) %>%
  summarise(Total = length(outcome))
```

Comparing the results with the published table, it looks like the values in
outcome are correct. Similarly, here is the published table for birthwgt_lb
value     label             Total
.         INAPPLICABLE      4449
0-5       UNDER 6 POUNDS    1125
6         6 POUNDS          2223
7         7 POUNDS          3049
8         8 POUNDS          1889
9-95      9 POUNDS OR MORE  799

```{r}
femPreg2002 %>%
  group_by(birthwgt_lb) %>%
  summarise(Total = length(birthwgt_lb))
```

The counts for 6, 7, and 8 pounds check out, and if you add up the counts
for 0-5 and 9-95, they check out, too. But if you look more closely, you will
notice one value that has to be an error, a 51 pound baby! This has been cleaned in the cleanFemPreg function.

### Interpretation
To work with data effectively, you have to think on two levels at the same time: the level of statistics and the level of context.
As an example, let's look at the sequence of outcomes for a few respondents.
This example looks up one respondent and prints a list of outcomes for her
pregnancies:
```{r}
CASEID = 10229
femPregCleaned %>%
  filter(caseid==CASEID) %>%
  .$outcome
```
The outcome code 1 indicates a live birth. Code 4 indicates a miscarriage; that is, a pregnancy that ended spontaneously, usually with no known medical cause.

Statistically this respondent is not unusual. Miscarriages are common and there are other respondents who reported as many or more. But remembering the context, this data tells the story of a woman who was pregnant six times, each time ending in miscarriage. Her seventh and most recent pregnancy ended in a live birth. If we consider this data with empathy,
it is natural to be moved by the story it tells.

Each record in the NSFG dataset represents a person who provided honest answers to many personal and difficult questions. We can use this data to answer statistical questions about family life, reproduction, and health. At the same time, we have an obligation to consider the people represented by the data, and to afford them respect and gratitude.

## Distributions
```{r}
#helper theme for common visualizations
ditch_the_axes <- theme(
  axis.text = element_blank(),
  axis.line = element_blank(),
  axis.ticks = element_blank(),
  panel.border = element_blank(),
  panel.grid = element_blank(),
  axis.title = element_blank()
  )
```
### Histograms
One of the best ways to describe a variable is to report the values that appear in the dataset and how many times each value appears. This description is called the distribution of the variable.
The most common representation of a distribution is a histogram, which is a graph that shows the frequency of each value. In this context, "frequency" means the number of times the value appears.

When you start working with a new dataset, we suggest you explore the variables you are planning to use one at a time, and a good way to start is by looking at histograms.

We transformed agepreg from centiyears to years, and combined birthwgt_lb and birthwgt_oz into a single quantity, totalwgt_lb. In this section we use these variables to demonstrate some features of histograms.

We start by reading the data and selecting records for live births:
```{r}
live <- femPregCleaned %>%
  filter(outcome==1)
```
Next we generate and plot the histogram of birthwgt_lb for live births.
```{r}
live %>%
  ggplot(mapping = aes(birthwgt_lb)) + 
  geom_histogram(bins = 16, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "pounds", y="Frequency")
```
The most common value, called the mode, is 7 pounds. The distribution is approximately bell-shaped, which is the shape of the normal distribution, also called a Gaussian distribution. But unlike a true normal distribution, this distribution is asymmetric; it has a tail that extends farther to the left than to the right.

Let's look at the histogram for birthwgt_oz:
```{r}
live %>%
  ggplot(mapping = aes(birthwgt_oz)) + 
  geom_histogram(bins = 16, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "ounces", y="Frequency")
```
In theory we expect this distribution to be uniform; that is, all values should have the same frequency. In fact, 0 is more common than the other values, and 1 and 15 are less common, probably because respondents round off birth weights that are close to an integer value.

Let's look at the histogram for agepreg:
```{r}
live %>%
  ggplot(mapping = aes(agepreg)) + 
  geom_histogram(bins = 45, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "years", y="Frequency")
```
The mode is 21 years. The distribution is very roughly bell-shaped, but in this case the tail extends farther to the right than left; most mothers are in their 20s, fewer in their 30s.

Let's look at the prglength:
```{r}
live %>%
  ggplot(mapping = aes(prglngth)) + 
  geom_histogram(bins = 50, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "weeks", y="Frequency")
```
By far the most common value is 39 weeks. The left tail is longer than the right; early babies are common, but pregnancies seldom go past 43 weeks, and doctors often intervene if they do.

### Outliers
Looking at histograms, it is easy to identify the most common values and the shape of the distribution, but rare values are not always visible. Before going on, it is a good idea to check for outliers, which are extreme values that might be errors in measurement and recording, or might be accurate reports of rare events.

```{r}
live['prglngth'] %>%
  distinct %>% 
  top_n(-10) %>%
  arrange(prglngth)
```
In the list of pregnancy lengths for live births, the 10 lowest values are [0, 4, 9, 13, 17, 18, 19, 20, 21, 22]. Values below 10 weeks are certainly errors; the most likely explanation is that the outcome was not coded correctly. Values higher than 30 weeks are probably legitimate. Between 10 and 30 weeks, it is hard to be sure; some values are probably errors, but some represent premature babies.

On the other end of the range, the highest values are:
```{r}
live['prglngth'] %>%
  distinct %>% 
  top_n(7) %>%
  arrange(prglngth)
```
Most doctors recommend induced labor if a pregnancy exceeds 42 weeks, so some of the longer values are surprising. In particular, 50 weeks seems medically unlikely.

The best way to handle outliers depends on "domain knowledge"; that is, information about where the data come from and what they mean. And it depends on what analysis you are planning to perform.
In this example, the motivating question is whether first babies tend to be early (or late). When people ask this question, they are usually interested in full-term pregnancies, so for this analysis we will focus on pregnancies longer than 27 weeks.
### First babies
Now we can compare the distribution of pregnancy lengths for first babies and others. We divide the data of live births using birthord to create a new column birthNumber, and compute their histograms:
```{r}
live['birthNumber'] <- if_else(live$birthord==1, 'first', 'other')
```

```{r}
live %>%
  ggplot(aes(x=prglngth, fill=birthNumber)) + 
  geom_histogram(bins=20, position="dodge", na.rm = TRUE, color='black') +
  labs(x="Weeks", y="Frequency") + 
  theme(legend.position = c(0.2, 0.7)) + 
  xlim(27, 46)
```
Histograms are useful because they make the most frequent values immediately apparent. But they are not the best choice for comparing two distributions. In this example, there are fewer "first babies" than "others," so some of the apparent differences in the histograms are due to sample sizes. In the next chapter we address this problem using probability mass functions.

### Effect Size
An effect size is a summary statistic intended to describe the size of an effect. For example, to describe the difference between two groups, one obvious choice is the difference in the means.
Mean pregnancy length for first babies is 38.601; for other babies it is 38.523. The difference is 0.078 weeks, which works out to 13 hours. As a fraction of the typical pregnancy length, this difference is about 0.2%. 
If we assume this estimate is accurate, such a difference would have no practical consequences. In fact, without observing a large number of pregnancies, it is unlikely that anyone would notice this difference at all.
Another way to convey the size of the effect is to compare the difference between groups to the variability within groups. Cohen's d is a statistic intended to do that; it is defined
d = (_x1_ - _x2_)/s
where _x1_ and _x2_ are the means of the groups and s is the "pooled standard
deviation".
```{r}
CohenEffectSize <- function(group1, group2){
  diff = mean(group1, na.rm = TRUE) - mean(group2, na.rm = TRUE)
  
  var1 = var(group1, na.rm=TRUE)
  var2 = var(group2, na.rm=TRUE)
  n1 = length(group1)
  n2 = length(group2)
  
  pooled_var = (n1*var1 + n2*var2)/(n1+n2)
  d = diff/sqrt(pooled_var)
  return (d)
}
```

Let's look at the effect size for difference in totalwgt_lb for first babies vs other babies. 
```{r}
firstBirthWgt <- live %>%
  filter(birthord==1) %>%
  .$totalwgt_lb

othersBirthWgt <- live %>%
  filter(birthord!=1) %>%
  .$totalwgt_lb
CohenEffectSize(firstBirthWgt, othersBirthWgt)
```

## Probability Mass Functions
By plotting the PMF instead of the histogram, we can compare the two distributions without being mislead by the difference in sample size.
```{r}
live %>%
  ggplot(aes(x=prglngth, ..density.., fill=birthNumber)) + 
  geom_histogram(bins = 20, stat="bin", position = "dodge", na.rm = TRUE, color='black') +
  labs(x="weeks", y="probability") + 
  theme(legend.position = c(0.2, 0.7)) + 
  xlim(27, 46)
```
```{r}
live %>%
  ggplot(aes(x=prglngth, ..density.., color=birthNumber)) +
  geom_step(binwidth = 0.7, stat="bin", position = "identity", na.rm = TRUE) +
  labs(x="weeks", y="probability") + 
  theme(legend.position = c(0.2, 0.7)) + 
  xlim(27, 46)
```

Based on this figure, first babies seem to be less likely than others to arrive on time (week 39) and more likely to be a late (weeks 41 and 42).

### Other visualizations
Histograms and PMFs are useful while you are exploring data and trying to identify patterns and relationships. Once you have an idea what is going on, a good next step is to design a visualization that makes the patterns you have identified as clear as possible.

In the NSFG data, the biggest differences in the distributions are near the mode. So it makes sense to zoom in on that part of the graph, and to transform the data to emphasize differences:

```{r}
#get the proportion of different number of weeks for prglngth, by order of birth
prglngthProp <- (table(live$prglngth, live$birthNumber) %>%
  prop.table(2) * 100 )%>%
  data.frame()
prglngthProp['weeks'] <- prglngthProp[, 'Var1'] %>%
  as.character %>%
  as.integer
```

```{r}
#filter for zooming from 35 to 46 weeks
p1 <- prglngthProp %>%
  subset(weeks>=35 & weeks< 46 & Var2=='first') %>%
  .[c('weeks', 'Freq')]
  
p2 <- prglngthProp %>%
  subset(weeks>=35 & weeks< 46 & Var2=='other') %>%
  .[c('weeks', 'Freq')]
```

```{r}
#find difference between the frequencies
diff <- within(merge(p1, p2, by='weeks'), {
  weeks <- weeks
  percent_points <- Freq.x - Freq.y
})[c('weeks', 'percent_points')]
```


```{r}
#plot the differences as column graph
diff %>%
  ggplot(aes(weeks, percent_points)) +
  geom_col(color='black', fill='purple') +
  labs(x="weeks", y="percentage points")
```
This figure makes the pattern clearer: first babies are less likely to be born in week 39, and somewhat more likely to be born in weeks 41 and 42.

## Cumulative Distribution Functions
### Limitations of PMF
PMFs work well if the number of values is small. But as the number of values increases, the probability associated with each value gets smaller and the effect of random noise increases.

```{r}
live %>%
  ggplot(aes(x=totalwgt_lb, ..density.., fill=birthOrder)) + 
  geom_histogram(bins=100, stat="bin", na.rm = TRUE, color='black') +
  labs(x="weight(pounds)", y="probability") + 
  theme(legend.position = c(0.2, 0.7)) 
```

Overall, these distributions resemble the bell shape of a normal distribution, with many values near the mean and a few values much higher and lower. But parts of this figure are hard to interpret. There are many spikes and valleys, and some apparent differences between the distributions. It is hard to tell which of these features are meaningful. Also, it is hard to see overall patterns; for example, which distribution do you think has the higher mean?

An alternative that avoids these problems is the cumulative distribution function (CDF).

### CDF
```{r}
live %>%
  ggplot(aes(x=prglngth)) +
  stat_ecdf(geom="step", na.rm = TRUE, color='blue') +
  labs(x="weeks", y="CDF") + 
  theme(legend.position = c(0.2, 0.7)) 
```

One way to read a CDF is to look up percentiles. For example, it looks like about 10% of pregnancies are shorter than 36 weeks, and about 90% are shorter than 41 weeks. 
The CDF also provides a visual representation of the shape of the distribution. Common values appear as steep or vertical sections of the CDF; in this example, the mode at 39 weeks is apparent. There are few values below 30 weeks, so the CDF in this range is flat.

### Comparing CDFs
CDFs are especially useful for comparing distributions. For example, here is the code that plots the CDF of birth weight for first babies and others.
```{r}
live %>%
  ggplot(aes(x=totalwgt_lb, color=birthOrder)) +
  stat_ecdf(geom="step", na.rm = TRUE) +
  labs(x="weight(pounds)", y="CDF") + 
  theme(legend.position = c(0.2, 0.7)) 
```
Compared to PMFs, this figure makes the shape of the distributions, and the differences between them, much clearer. We can see that first babies are slightly lighter throughout the distribution, with a larger discrepancy above the mean.

### Percentile-based statistics
Once you have computed a CDF, it is easy to compute percentiles and percentile ranks. Percentile can be used to compute percentile-based summary statistics. For example, the 50th percentile is the value that divides the distribution in half, also known as the median.
```{r}
#50th percentile of pregnancy lengths
live$prglngth %>%
  quantile(.50, na.rm = TRUE)
#different quartiles of total birth weight
live$totalwgt_lb %>%
  summary
```

### Random Numbers
Suppose we choose a random sample from the population of live births and look up the percentile rank of their birth weights. Now suppose we compute the CDF of the percentile ranks. What do you think the distribution will look like?
```{r}
live[sample(nrow(live), 100, replace = TRUE),] %>% 
 mutate(percrank=rank(totalwgt_lb)/length(totalwgt_lb)) %>%
  ggplot(aes(x=percrank)) +
  stat_ecdf(geom="step", na.rm = TRUE, color='blue') +
  labs(x="precentile rank", y="CDF")
```
The CDF is approximately a straight line, which means that the distribution is uniform.

```{r}
live[sample(nrow(live), 100, replace = TRUE),] %>% 
 mutate(percrank=rank(totalwgt_lb)/length(totalwgt_lb)) %>%
  ggplot(aes(x=percrank, ..density..)) + 
  geom_histogram(stat="bin", na.rm = TRUE, color='black', fill='light blue') +
  labs(x="percentile rank", y="probability")
```

That outcome might be non-obvious, but it is a consequence of the way the CDF is defined. What this figure shows is that 10% of the sample is below the 10th percentile, 20% is below the 20th percentile, and so on, exactly as we should expect. So, regardless of the shape of the CDF, the distribution of percentile ranks is uniform.

## Comparing percentile ranks
Percentile ranks are useful for comparing measurements across different groups. For example, people who compete in foot races are usually grouped by age and gender. To compare people in different age groups, you can convert race times to percentile ranks.

## Modeling Distributions
The distributions we have used so far are called empirical distributions because they are based on empirical observations, which are necessarily finite samples.
The alternative is an analytic distribution, which is characterized by a CDF that is a mathematical function. Analytic distributions can be used to model empirical distributions. In this context, a model is a simplification that leaves out unneeded details. This chapter presents common analytic distributions and uses them to model data from a variety of sources.

### Exponential Distribution
The CDF of the exponential distribution is: CDF(x) = 1 - e^(-λ*x). The parameter, λ, determines the shape of the distribution.
```{r}
data.frame(x = c(0,3.0)) %>%
  ggplot(mapping = aes(x)) +
  stat_function(fun=pexp, args=list(rate=0.5), aes(colour='0.5')) + 
  stat_function(fun=pexp, args=list(rate=1), aes(colour='1')) + 
  stat_function(fun=pexp, args=list(rate=2), aes(colour='2')) + 
  #scale_color_manual('λ', values = c('green', 'light blue', 'blue')) +
  labs(x="x", y="CDF")
```
This figure shows how the CDF of exponential distribution for lambda = 0.5, 1, and 2 looks like.

In the real world, exponential distributions come up when we look at a series of events and measure the times between events, called interarrival times. *If the events are equally likely to occur at any time, the distribution of interarrival times tends to look like an exponential distribution.*

As an example, we will look at the interarrival time of births. On December 18, 1997, 44 babies were born in a hospital in Brisbane, Australia. The time of birth for all 44 babies was reported in the local paper; the complete dataset is in a file called babyboom.dat, in the ThinkStats2 repository. It has 4 columns time, sex, weight_g, and minutes, where minutes is time of birth converted to minutes since midnight.
```{r}
babyBoom <- read.fwf('~/Documents/CodeWork/ThinkStats2/code/babyboom.dat',skip = 59, widths = c(8, 8, 8, 8), col.names = c('time', 'sex', 'weight_g', 'minutes'))
```


```{r}
babyBoom %>%
  mutate(diffs = minutes - lag(minutes)) %>%
  ggplot(mapping=aes(diffs)) +
  geom_step(aes(y=..y..), stat='ecdf', na.rm=TRUE, color='blue') +
  labs(x='minutes', y='CDF')
```

This figure shows the CDF of interarrival times. It seems to have the general shape of an exponential distribution, but how can we tell?
One way is to plot the complementary CDF, which is 1 - CDF(x), on a log-y scale. For data from an exponential distribution, the result is a straight line.

```{r}
babyBoom %>%
  mutate(diffs = minutes - lag(minutes)) %>%
  ggplot(mapping=aes(diffs)) +
  geom_step(aes(y=log10(1-..y..)), stat='ecdf', na.rm=TRUE, color='blue') +
  labs(x='minutes', y='CCDF')
```
It is not exactly straight, which indicates that the exponential distribution is not a perfect model for this data. *Most likely the underlying assumption -- that a birth is equally likely at any time of day -- is not exactly true.* Nevertheless, it might be reasonable to model this dataset with an exponential distribution. With that simplification, we can summarize the distribution with a single parameter.

The parameter, λ, can be interpreted as a rate; that is, the number of events
that occur, on average, in a unit of time. In this example, 44 babies are
born in 24 hours, so the rate is λ = 0.0306 births per minute. The mean of
an exponential distribution is 1/λ, so the mean time between births is 32.7
minutes.

### Normal Distribution
The normal distribution with μ = 0 and σ = 1 is called the standard normal distribution.
```{r}
data.frame(x = c(-1, 4)) %>%
  ggplot(mapping = aes(x)) +
  stat_function(fun=pnorm, args=list(mean=1, sd=0.5), aes(colour='mean=1, sd=0.5')) + 
  stat_function(fun=pnorm, args=list(mean=2, sd=0.4), aes(colour='mean=2, sd=0.4')) + 
  stat_function(fun=pnorm, args=list(mean=3, sd=0.3), aes(colour='mean=3, sd=0.3')) + 
  #scale_color_manual('mean, sd', values = c('green', 'red', 'blue')) +
  labs(x="x", y="CDF")
```
This figure shows CDFs for normal distributions with a range of parameters. The sigmoid shape of these curves is a recognizable characteristic of a normal distribution.

We earlier looked at the distribution of birth weights. Next figure shows the empirical CDF of weights for all live births and the CDF of a normal distribution with the same mean and variance.
```{r}
live$totalwgt_lb %>%
  mean(na.rm=TRUE)
live$totalwgt_lb %>%
  sd(na.rm=TRUE)
```
```{r}
live %>%
  ggplot(aes(x=totalwgt_lb)) +
  stat_ecdf(geom="step", na.rm = TRUE, aes(color='data')) +
  stat_function(fun=pnorm, args=list(mean=7.28, sd=1.24), aes(color='model')) +
  labs(x="weight(pounds)", y="CDF") + 
  scale_color_manual('CDF', values = c('blue', 'red')) +
  theme(legend.position = c(0.8, 0.2)) 
```
The normal distribution is a good model for this dataset, so if we summarize the distribution with the parameters μ = 7.28 and σ = 1.24, the resulting error (difference between the model and the data) is small.

Below the 10th percentile there is a discrepancy between the data and the model; there are more light babies than we would expect in a normal distribution. If we are specifically interested in preterm babies, it would be important to get this part of the distribution right, so it might not be appropriate to use the normal model.

### Normal Probability Plot
For the exponential distribution, and a few others, there are simple transformations we can use to test whether an analytic distribution is a good model for a dataset.
For the normal distribution there is no such transformation, but there is an alternative called a normal probability plot.
```{r}
ggplot()+
  geom_qq(aes(sample = rnorm(400, mean = 0, sd = 1), color='mean = 0, sd = 1'), 
          geom='line') +
  geom_qq(aes(sample = rnorm(400, mean = 1, sd = 1), color='mean = 1, sd = 1'), 
          geom='line') +
  geom_qq(aes(sample = rnorm(400, mean = 5, sd = 2), color='mean = 5, sd = 2'), 
          geom='line') + 
  scale_color_manual('Parameters', values = c('blue', 'red', 'green')) +
  labs(x='standard normal sample', y='sample values') +
  theme(legend.position = c(0.2, 0.7)) 
```
*If the distribution of the sample is approximately normal, the result is a
straight line with intercept mu and slope sigma.* In the figure above, values are generated from normal distributions with different values of mu and sigma. The lines are approximately straight, with values in the tails deviating more than values near the mean.

Now let's try it with real data. We will generate the normal probability plot for the birth weight data.
```{r}
ggplot()+
  #taking all live births
  stat_qq(aes(sample=live$totalwgt_lb, color='all live'), 
          na.rm = TRUE, geom="line", distribution = qnorm) +
  #taking live births with full term only
  stat_qq(aes(sample=live$totalwgt_lb[live$prglngth>36], color='full term'), 
          na.rm = TRUE, geom="line", distribution = qnorm) +
  #the model as per mu and sigma
  geom_qq(aes(sample = rnorm(length(live$totalwgt_lb), mean = 7.4, sd = 1.2), 
              color='model'), geom='line') +
  scale_color_manual('Cases', values = c('blue', 'light blue', 'red')) +
  labs(x='Standard Deviations from mean', y='Birth weight(pounds)') +
  theme(legend.position = c(0.2, 0.7)) 
```
This figure shows the results for all live births, and also for full term births (pregnancy length greater than 36 weeks). Both curves match the model near the mean and deviate in the tails. The heaviest babies are heavier than what the model expects, and the lightest babies are lighter.
When we select only full term births, we remove some of the lightest weights, which reduces the discrepancy in the lower tail of the distribution. 
This plot suggests that the normal model describes the distribution well within a few standard deviations from the mean, but not in the tails. Whether it is good enough for practical purposes depends on the purposes.

### Lognormal distribution
If the logarithms of a set of values have a normal distribution, the values have a lognormal distribution. The CDF of the lognormal distribution is the same as the CDF of the normal distribution, with log x substituted for x.
The parameters of the lognormal distribution are usually denoted mu and sigma. But remember that these parameters are not the mean and standard deviation.

If a sample is approximately lognormal and you plot its CDF on a log-x scale, it will have the characteristic shape of a normal distribution. To test how well the sample fits a lognormal model, you can make a normal probability plot using the log of the values in the sample.
As an example, let's look at the distribution of adult weights, which is approximately lognormal. 
The National Center for Chronic Disease Prevention and Health Promotion conducts an annual survey as part of the Behavioral Risk Factor Surveillance System (BRFSS). In 2008, they interviewed 414,509 respondents and asked about their demographics, health, and health risks. Among the data they collected are the weights in kilograms of 398,484 respondents.

```{r}
adultWeight <- read.fwf('~/Documents/CodeWork/ThinkStats2/code/CDBRFS08.ASC.gz', 
                        widths = c(101, 2, 24, 4, 12, 1, 655, 9, 442, 3, 5), 
                        col.names = c('v1', 'age', 'v2', 'wtyrago', 'v3', 'sex', 'v4', 'finalwt', 'v5', 'htm3', 'wtkg2'))
```

```{r}
#clean weight
adultWeight$wtkg2[adultWeight$wtkg2==99999] <- NA
adultWeight$wtkg2 <- adultWeight$wtkg2/100.0

# clean weight a year ago
adultWeight$wtyrago[adultWeight$wtyrago %in% c(7777,9999)] <- NA
adultWeight['wtyrago'] <- if_else(adultWeight$wtyrago < 9000, 
                                  adultWeight$wtyrago/2.2, 
                                  adultWeight$wtyrago-9000)
```

```{r}
adultWeight['wtyrago']
```


```{r}
adultWeight %>%
  ggplot(aes(x=wtyrago)) +
  stat_ecdf(geom="step", na.rm = TRUE, aes(color='data'))
  #taking live births with full term only
  #geom_qq(aes(sample=adultWeight$wtyrago, color='full term'), 
  #        na.rm = TRUE, geom="line") #+
  #the model as per mu and sigma
  #geom_qq(aes(sample = rnorm(length(live$totalwgt_lb), mean = 7.4, sd = 1.2), 
   #           color='model'), geom='line')
  #geom_qq(aes(sample = rlogis(100)), 
    #      geom='line')
```

