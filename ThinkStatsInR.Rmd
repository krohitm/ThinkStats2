---
title: "thinkstats"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(extraDistr)
library(tidyverse)
#library(plotly)
```

The data source is obtained from https://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm
This contains fixed width files, and stata dictionaries consisting of columns for the data files
```{r}
#helper function to parse Stata dictionary
dct.parser <- function(dct, includes = c("StartPos", "StorageType", "ColName", 
                                         "ColWidth", "VarLabel"),
                       preview = FALSE) {
  temp <- readLines(dct)
  temp <- temp[grepl("_column", temp)]
  
  if (isTRUE(preview)) {
    head(temp)
  } else {
    possibilities <- c("StartPos", "StorageType", 
                       "ColName", "ColWidth", "VarLabel")
    classes <- c("numeric", "character", "
                 character", "numeric", "character")
    pattern <- c(StartPos = ".*\\(([0-9 ]+)\\)",
                 StorageType = "(byte|int|long|float|double|str[0-9]+)",
                 ColName = "(.*)",
                 ColWidth = "%([0-9.]+)[a-z]+",
                 VarLabel = "(.*)")
    
    mymatch <- match(includes, possibilities)
    
    pattern <- paste(paste(pattern[mymatch], 
                           collapse ="\\s+"), "$", sep = "")    
    
    metadata <- setNames(lapply(seq_along(mymatch), function(x) {
      out <- gsub(pattern, paste("\\", x, sep = ""), temp)
      out <- gsub("^\\s+|\\s+$", "", out)
      out <- gsub('\"', "", out, fixed = TRUE)
      class(out) <- classes[mymatch][x] ; out }), 
                         possibilities[mymatch])
    
    implicit.dec <- grepl("\\.[1-9]", metadata[["ColWidth"]])
    if (any(implicit.dec)) {
      message("Some variables may need to be corrected for implicit decimals. 
              Try 'MESSAGES(output_from_dct.parser)' for more details.")
      metadata[["Decimals"]] <- rep(NA, length(metadata[["ColWidth"]]))
      metadata[["Decimals"]][implicit.dec] <-
        as.numeric(gsub("[0-9]+\\.", "", 
                        metadata[["ColWidth"]][implicit.dec]))
      metadata[["ColWidth"]] <- floor(as.numeric(metadata[["ColWidth"]]))
    }
    
    metadata[["ColName"]] <- make.names(
      gsub("\\s", "", metadata[["ColName"]]))
    
    metadata <- data.frame(metadata)
    
    if ("StorageType" %in% includes) {
      metadata <- 
        within(metadata, {
          colClasses <- ifelse(
            StorageType == "byte", "raw",
            ifelse(StorageType %in% c("double", "long", "float"), 
                   "numeric", 
                   ifelse(StorageType == "int", "integer",
                          ifelse(substr(StorageType, 1, 3) == "str", 
                                 "character", NA))))
        })
    }
    if (any(implicit.dec)) {
      attr(metadata, "MESSAGE") <- c(sprintf("%s", paste(
        "Some variables might need to be corrected for implicit decimals. 
        A variable, 'Decimals', has been created in the metadata that
        indicates the number of decimal places the variable should hold. 
        To correct the output, try (where your stored output is 'mydf'): 
        
        lapply(seq_along(mydf[!is.na(Decimals)]), 
        function(x) mydf[!is.na(Decimals)][x]
        / 10^Decimals[!is.na(Decimals)][x])
        
        The variables in question are:
        ")), sprintf("%s", metadata[["ColName"]][!is.na(metadata[["Decimals"]])]))
            class(attr(metadata, "MESSAGE")) <- c(
                "MESSAGE", class(attr(metadata, "MESSAGE")))
        }
        attr(metadata, "original.dictionary") <- 
            c(dct, basename(dct))
        metadata
    }
}
```


We can read the coulmns from 2002FemPreg.dct and use those columns to import the data from the fixed width file 2002FemPreg.dat
```{r}
femPreg2002columns <- dct.parser('~/Documents/CodeWork/ThinkStats2/code/2002FemPreg.dct')
femPreg2002 <- read.fwf('~/Documents/CodeWork/ThinkStats2/code/2002FemPreg.dat', widths = femPreg2002columns$ColWidth, col.names = femPreg2002columns$ColName)
```

Taking a look at the data
```{r}
head(femPreg2002)
```

We can see a lot of missing values. We'll clean the data for the columns that we want to analyze.

## Transformation

1. agepreg contains the mother's age at the end of the pregnancy. In the data file, agepreg is encoded as an integer number of centiyears. So the first line divides each element of agepreg by 100, yielding a floating-point value in years.

2. birthwgt_lb and birthwgt_oz contain the weight of the baby, in pounds and ounces, for pregnancies that end in live birth. In addition it uses several special codes:
  97 NOT ASCERTAINED
  98 REFUSED
  99 DONT KNOW
Special values encoded as numbers are dangerous because if they are not
handled properly, they can generate bogus results, like a 99-pound baby. Assuming that a baby can't be generally more than 20 lb at birth, we will replace all other values with NA, as they are NOT ASCERTAINED(97),  REFUSED(98), DONT KNOW(99), or invalid values.
Similarly, the age of father has these similar special codes, which we will replace by NA
```{r}
cleanFemPreg <- function(data){
  # mother's age is encoded in centiyears; convert to years
  data['agepreg'] <-  data['agepreg']/100.0
  
  # birthwgt_lb contains at least one bogus value (51 lbs)
  # replace with NaN
  data$birthwgt_lb[data$birthwgt_lb > 20] <- NA
  
  # replace 'not ascertained', 'refused', 'don't know' with NA
  na_vals = c(97, 98, 99)
  data$birthwgt_oz[data$birthwgt_oz %in% na_vals] <- NA
  data$hpagelb[data$hpagelb %in% na_vals] <- NA
  
  # birthweight is stored in two columns, lbs and oz.
  # convert to a single column in lb
  data['totalwgt_lb'] <- data$birthwgt_lb + (data$birthwgt_oz / 16.0)
  
  return (data)
}
```

```{r}
femPregCleaned <- cleanFemPreg(femPreg2002)
```

### Validation
One way to validate data is to compute basic statistics and compare them with published results. For example, the NSFG codebook includes tables that summarize each variable. Here is the table for outcome, which encodes the outcome of each pregnancy:
value     label         Total
1         LIVE BIRTH        9148
2         INDUCED ABORTION  1862
3         STILLBIRTH        120
4         MISCARRIAGE       1921
5         ECTOPIC PREGNANCY 190
6         CURRENT PREGNANCY 352

```{r}
femPreg2002 %>%
  group_by(outcome) %>%
  summarise(Total = length(outcome))
```

Comparing the results with the published table, it looks like the values in
outcome are correct. Similarly, here is the published table for birthwgt_lb
value     label             Total
.         INAPPLICABLE      4449
0-5       UNDER 6 POUNDS    1125
6         6 POUNDS          2223
7         7 POUNDS          3049
8         8 POUNDS          1889
9-95      9 POUNDS OR MORE  799

```{r}
femPreg2002 %>%
  group_by(birthwgt_lb) %>%
  summarise(Total = length(birthwgt_lb))
```

The counts for 6, 7, and 8 pounds check out, and if you add up the counts
for 0-5 and 9-95, they check out, too. But if you look more closely, you will
notice one value that has to be an error, a 51 pound baby! This has been cleaned in the cleanFemPreg function.

### Interpretation
To work with data effectively, you have to think on two levels at the same time: the level of statistics and the level of context.
As an example, let's look at the sequence of outcomes for a few respondents.
This example looks up one respondent and prints a list of outcomes for her
pregnancies:
```{r}
CASEID = 10229
femPregCleaned %>%
  filter(caseid==CASEID) %>%
  .$outcome
```
The outcome code 1 indicates a live birth. Code 4 indicates a miscarriage; that is, a pregnancy that ended spontaneously, usually with no known medical cause.

Statistically this respondent is not unusual. Miscarriages are common and there are other respondents who reported as many or more. But remembering the context, this data tells the story of a woman who was pregnant six times, each time ending in miscarriage. Her seventh and most recent pregnancy ended in a live birth. If we consider this data with empathy,
it is natural to be moved by the story it tells.

Each record in the NSFG dataset represents a person who provided honest answers to many personal and difficult questions. We can use this data to answer statistical questions about family life, reproduction, and health. At the same time, we have an obligation to consider the people represented by the data, and to afford them respect and gratitude.

## Distributions
```{r}
#helper theme for common visualizations
ditch_the_axes <- theme(
  axis.text = element_blank(),
  axis.line = element_blank(),
  axis.ticks = element_blank(),
  panel.border = element_blank(),
  panel.grid = element_blank(),
  axis.title = element_blank()
  )
```
### Histograms
One of the best ways to describe a variable is to report the values that appear in the dataset and how many times each value appears. This description is called the distribution of the variable.
The most common representation of a distribution is a histogram, which is a graph that shows the frequency of each value. In this context, "frequency" means the number of times the value appears.

When you start working with a new dataset, we suggest you explore the variables you are planning to use one at a time, and a good way to start is by looking at histograms.

We transformed agepreg from centiyears to years, and combined birthwgt_lb and birthwgt_oz into a single quantity, totalwgt_lb. In this section we use these variables to demonstrate some features of histograms.

We start by reading the data and selecting records for live births:
```{r}
live <- femPregCleaned %>%
  filter(outcome==1)
```
Next we generate and plot the histogram of birthwgt_lb for live births.
```{r}
live %>%
  ggplot(mapping = aes(birthwgt_lb)) + 
  geom_histogram(bins = 16, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "pounds", y="Frequency")
```
The most common value, called the mode, is 7 pounds. The distribution is approximately bell-shaped, which is the shape of the normal distribution, also called a Gaussian distribution. But unlike a true normal distribution, this distribution is asymmetric; it has a tail that extends farther to the left than to the right.

Let's look at the histogram for birthwgt_oz:
```{r}
live %>%
  ggplot(mapping = aes(birthwgt_oz)) + 
  geom_histogram(bins = 16, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "ounces", y="Frequency")
```
In theory we expect this distribution to be uniform; that is, all values should have the same frequency. In fact, 0 is more common than the other values, and 1 and 15 are less common, probably because respondents round off birth weights that are close to an integer value.

Let's look at the histogram for agepreg:
```{r}
live %>%
  ggplot(mapping = aes(agepreg)) + 
  geom_histogram(bins = 45, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "years", y="Frequency")
```
The mode is 21 years. The distribution is very roughly bell-shaped, but in this case the tail extends farther to the right than left; most mothers are in their 20s, fewer in their 30s.

Let's look at the prglength:
```{r}
live %>%
  ggplot(mapping = aes(prglngth)) + 
  geom_histogram(bins = 50, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "weeks", y="Frequency")
```
By far the most common value is 39 weeks. The left tail is longer than the right; early babies are common, but pregnancies seldom go past 43 weeks, and doctors often intervene if they do.

### Outliers
Looking at histograms, it is easy to identify the most common values and the shape of the distribution, but rare values are not always visible. Before going on, it is a good idea to check for outliers, which are extreme values that might be errors in measurement and recording, or might be accurate reports of rare events.

```{r}
live['prglngth'] %>%
  distinct %>% 
  top_n(-10) %>%
  arrange(prglngth)
```
In the list of pregnancy lengths for live births, the 10 lowest values are [0, 4, 9, 13, 17, 18, 19, 20, 21, 22]. Values below 10 weeks are certainly errors; the most likely explanation is that the outcome was not coded correctly. Values higher than 30 weeks are probably legitimate. Between 10 and 30 weeks, it is hard to be sure; some values are probably errors, but some represent premature babies.

On the other end of the range, the highest values are:
```{r}
live['prglngth'] %>%
  distinct %>% 
  top_n(7) %>%
  arrange(prglngth)
```
Most doctors recommend induced labor if a pregnancy exceeds 42 weeks, so some of the longer values are surprising. In particular, 50 weeks seems medically unlikely.

The best way to handle outliers depends on "domain knowledge"; that is, information about where the data come from and what they mean. And it depends on what analysis you are planning to perform.
In this example, the motivating question is whether first babies tend to be early (or late). When people ask this question, they are usually interested in full-term pregnancies, so for this analysis we will focus on pregnancies longer than 27 weeks.
### First babies
Now we can compare the distribution of pregnancy lengths for first babies and others. We divide the data of live births using birthord to create a new column birthNumber, and compute their histograms:
```{r}
live['birthNumber'] <- if_else(live$birthord==1, 'first', 'other')
```

```{r}
live %>%
  ggplot(aes(x=prglngth, fill=birthNumber)) + 
  geom_histogram(bins=20, position="dodge", na.rm = TRUE, color='black') +
  labs(x="Weeks", y="Frequency") + 
  theme(legend.position = c(0.2, 0.7)) + 
  xlim(27, 46)
```
Histograms are useful because they make the most frequent values immediately apparent. But they are not the best choice for comparing two distributions. In this example, there are fewer "first babies" than "others," so some of the apparent differences in the histograms are due to sample sizes. In the next chapter we address this problem using probability mass functions.

### Effect Size
An effect size is a summary statistic intended to describe the size of an effect. For example, to describe the difference between two groups, one obvious choice is the difference in the means.
Mean pregnancy length for first babies is 38.601; for other babies it is 38.523. The difference is 0.078 weeks, which works out to 13 hours. As a fraction of the typical pregnancy length, this difference is about 0.2%. 
If we assume this estimate is accurate, such a difference would have no practical consequences. In fact, without observing a large number of pregnancies, it is unlikely that anyone would notice this difference at all.
Another way to convey the size of the effect is to compare the difference between groups to the variability within groups. Cohen's d is a statistic intended to do that; it is defined
d = (_x1_ - _x2_)/s
where _x1_ and _x2_ are the means of the groups and s is the "pooled standard
deviation".
```{r}
CohenEffectSize <- function(group1, group2){
  diff = mean(group1, na.rm = TRUE) - mean(group2, na.rm = TRUE)
  
  var1 = var(group1, na.rm=TRUE)
  var2 = var(group2, na.rm=TRUE)
  n1 = length(group1)
  n2 = length(group2)
  
  pooled_var = (n1*var1 + n2*var2)/(n1+n2)
  d = diff/sqrt(pooled_var)
  return (d)
}
```

Let's look at the effect size for difference in totalwgt_lb for first babies vs other babies. 
```{r}
firstBirthWgt <- live %>%
  filter(birthord==1) %>%
  .$totalwgt_lb

othersBirthWgt <- live %>%
  filter(birthord!=1) %>%
  .$totalwgt_lb
CohenEffectSize(firstBirthWgt, othersBirthWgt)
```

## Probability Mass Functions
By plotting the PMF instead of the histogram, we can compare the two distributions without being mislead by the difference in sample size.
```{r}
live %>%
  ggplot(aes(x=prglngth, ..density.., fill=birthNumber)) + 
  geom_histogram(bins = 20, stat="bin", position = "dodge", na.rm = TRUE, color='black') +
  labs(x="weeks", y="probability") + 
  theme(legend.position = c(0.2, 0.7)) + 
  xlim(27, 46)
```
```{r}
live %>%
  ggplot(aes(x=prglngth, ..density.., color=birthNumber)) +
  geom_step(binwidth = 0.7, stat="bin", position = "identity", na.rm = TRUE) +
  labs(x="weeks", y="probability") + 
  theme(legend.position = c(0.2, 0.7)) + 
  xlim(27, 46)
```

Based on this figure, first babies seem to be less likely than others to arrive on time (week 39) and more likely to be a late (weeks 41 and 42).

### Other visualizations
Histograms and PMFs are useful while you are exploring data and trying to identify patterns and relationships. Once you have an idea what is going on, a good next step is to design a visualization that makes the patterns you have identified as clear as possible.

In the NSFG data, the biggest differences in the distributions are near the mode. So it makes sense to zoom in on that part of the graph, and to transform the data to emphasize differences:

```{r}
#get the proportion of different number of weeks for prglngth, by order of birth
prglngthProp <- (table(live$prglngth, live$birthNumber) %>%
  prop.table(2) * 100 )%>%
  data.frame()
prglngthProp['weeks'] <- prglngthProp[, 'Var1'] %>%
  as.character %>%
  as.integer
```

```{r}
#filter for zooming from 35 to 46 weeks
p1 <- prglngthProp %>%
  subset(weeks>=35 & weeks< 46 & Var2=='first') %>%
  .[c('weeks', 'Freq')]
  
p2 <- prglngthProp %>%
  subset(weeks>=35 & weeks< 46 & Var2=='other') %>%
  .[c('weeks', 'Freq')]
```

```{r}
#find difference between the frequencies
diff <- within(merge(p1, p2, by='weeks'), {
  weeks <- weeks
  percent_points <- Freq.x - Freq.y
})[c('weeks', 'percent_points')]
```


```{r}
#plot the differences as column graph
diff %>%
  ggplot(aes(weeks, percent_points)) +
  geom_col(color='black', fill='purple') +
  labs(x="weeks", y="percentage points")
```
This figure makes the pattern clearer: first babies are less likely to be born in week 39, and somewhat more likely to be born in weeks 41 and 42.

## Cumulative Distribution Functions
### Limitations of PMF
PMFs work well if the number of values is small. But as the number of values increases, the probability associated with each value gets smaller and the effect of random noise increases.

```{r}
live %>%
  ggplot(aes(x=totalwgt_lb, ..density.., fill=birthNumber)) + 
  geom_histogram(bins=100, stat="bin", na.rm = TRUE, color='black') +
  labs(x="weight(pounds)", y="probability") + 
  theme(legend.position = c(0.2, 0.7)) 
```

Overall, these distributions resemble the bell shape of a normal distribution, with many values near the mean and a few values much higher and lower. But parts of this figure are hard to interpret. There are many spikes and valleys, and some apparent differences between the distributions. It is hard to tell which of these features are meaningful. Also, it is hard to see overall patterns; for example, which distribution do you think has the higher mean?

An alternative that avoids these problems is the cumulative distribution function (CDF).

### CDF
```{r}
live %>%
  ggplot(aes(x=prglngth)) +
  stat_ecdf(geom="step", na.rm = TRUE, color='blue') +
  labs(x="weeks", y="CDF") + 
  theme(legend.position = c(0.2, 0.7)) 
```

One way to read a CDF is to look up percentiles. For example, it looks like about 10% of pregnancies are shorter than 36 weeks, and about 90% are shorter than 41 weeks. 
The CDF also provides a visual representation of the shape of the distribution. Common values appear as steep or vertical sections of the CDF; in this example, the mode at 39 weeks is apparent. There are few values below 30 weeks, so the CDF in this range is flat.

### Comparing CDFs
CDFs are especially useful for comparing distributions. For example, here is the code that plots the CDF of birth weight for first babies and others.
```{r}
live %>%
  ggplot(aes(x=totalwgt_lb, color=birthNumber)) +
  stat_ecdf(geom="step", na.rm = TRUE) +
  labs(x="weight(pounds)", y="CDF") + 
  theme(legend.position = c(0.2, 0.7)) 
```
Compared to PMFs, this figure makes the shape of the distributions, and the differences between them, much clearer. We can see that first babies are slightly lighter throughout the distribution, with a larger discrepancy above the mean.

### Percentile-based statistics
Once you have computed a CDF, it is easy to compute percentiles and percentile ranks. Percentile can be used to compute percentile-based summary statistics. For example, the 50th percentile is the value that divides the distribution in half, also known as the median.
```{r}
#50th percentile of pregnancy lengths
live$prglngth %>%
  quantile(.50, na.rm = TRUE)
#different quartiles of total birth weight
live$totalwgt_lb %>%
  summary
```

### Random Numbers
Suppose we choose a random sample from the population of live births and look up the percentile rank of their birth weights. Now suppose we compute the CDF of the percentile ranks. What do you think the distribution will look like?
```{r}
live[sample(nrow(live), 100, replace = TRUE),] %>% 
 mutate(percrank=rank(totalwgt_lb)/length(totalwgt_lb)) %>%
  ggplot(aes(x=percrank)) +
  stat_ecdf(geom="step", na.rm = TRUE, color='blue') +
  labs(x="precentile rank", y="CDF")
```
The CDF is approximately a straight line, which means that the distribution is uniform.

```{r}
live[sample(nrow(live), 100, replace = TRUE),] %>% 
 mutate(percrank=rank(totalwgt_lb)/length(totalwgt_lb)) %>%
  ggplot(aes(x=percrank, ..density..)) + 
  geom_histogram(stat="bin", na.rm = TRUE, color='black', fill='light blue') +
  labs(x="percentile rank", y="probability")
```

That outcome might be non-obvious, but it is a consequence of the way the CDF is defined. What this figure shows is that 10% of the sample is below the 10th percentile, 20% is below the 20th percentile, and so on, exactly as we should expect. So, regardless of the shape of the CDF, the distribution of percentile ranks is uniform.

## Comparing percentile ranks
Percentile ranks are useful for comparing measurements across different groups. For example, people who compete in foot races are usually grouped by age and gender. To compare people in different age groups, you can convert race times to percentile ranks.

## Modeling Distributions
The distributions we have used so far are called empirical distributions because they are based on empirical observations, which are necessarily finite samples.
The alternative is an analytic distribution, which is characterized by a CDF that is a mathematical function. Analytic distributions can be used to model empirical distributions. In this context, a model is a simplification that leaves out unneeded details. This chapter presents common analytic distributions and uses them to model data from a variety of sources.

### Exponential Distribution
The CDF of the exponential distribution is: CDF(x) = 1 - e^(-λ*x). The parameter, λ, determines the shape of the distribution.
```{r}
data.frame(x = c(0,3.0)) %>%
  ggplot(mapping = aes(x)) +
  stat_function(fun=pexp, args=list(rate=0.5), aes(colour='0.5')) + 
  stat_function(fun=pexp, args=list(rate=1), aes(colour='1')) + 
  stat_function(fun=pexp, args=list(rate=2), aes(colour='2')) + 
  scale_color_manual('λ', values = c('green', 'red', 'blue')) +
  labs(x="x", y="CDF")
```
This figure shows how the CDF of exponential distribution for lambda = 0.5, 1, and 2 looks like.

In the real world, exponential distributions come up when we look at a series of events and measure the times between events, called interarrival times. *If the events are equally likely to occur at any time, the distribution of interarrival times tends to look like an exponential distribution.*

As an example, we will look at the interarrival time of births. On December 18, 1997, 44 babies were born in a hospital in Brisbane, Australia. The time of birth for all 44 babies was reported in the local paper; the complete dataset is in a file called babyboom.dat, in the ThinkStats2 repository. It has 4 columns time, sex, weight_g, and minutes, where minutes is time of birth converted to minutes since midnight.
```{r}
babyBoom <- read.fwf('~/Documents/CodeWork/ThinkStats2/code/babyboom.dat',skip = 59, widths = c(8, 8, 8, 8), col.names = c('time', 'sex', 'weight_g', 'minutes'))
```


```{r}
babyBoom %>%
  mutate(diffs = minutes - lag(minutes)) %>%
  ggplot(mapping=aes(diffs)) +
  geom_step(aes(y=..y..), stat='ecdf', na.rm=TRUE, color='blue') +
  labs(x='minutes', y='CDF')
```

This figure shows the CDF of interarrival times. It seems to have the general shape of an exponential distribution, but how can we tell?
One way is to plot the complementary CDF, which is 1 - CDF(x), on a log-y scale. For data from an exponential distribution, the result is a straight line.

```{r}
babyBoom %>%
  mutate(diffs = minutes - lag(minutes)) %>%
  ggplot(mapping=aes(diffs)) +
  geom_step(aes(y=log10(1-..y..)), stat='ecdf', na.rm=TRUE, color='blue') +
  labs(x='minutes', y='CCDF')
```
It is not exactly straight, which indicates that the exponential distribution is not a perfect model for this data. *Most likely the underlying assumption -- that a birth is equally likely at any time of day -- is not exactly true.* Nevertheless, it might be reasonable to model this dataset with an exponential distribution. With that simplification, we can summarize the distribution with a single parameter.

The parameter, λ, can be interpreted as a rate; that is, the number of events
that occur, on average, in a unit of time. In this example, 44 babies are
born in 24 hours, so the rate is λ = 0.0306 births per minute. The mean of
an exponential distribution is 1/λ, so the mean time between births is 32.7
minutes.

### Normal Distribution
The normal distribution with μ = 0 and σ = 1 is called the standard normal distribution.
```{r}
data.frame(x = c(-1, 4)) %>%
  ggplot(mapping = aes(x)) +
  stat_function(fun=pnorm, args=list(mean=1, sd=0.5), aes(colour='mean=1, sd=0.5')) + 
  stat_function(fun=pnorm, args=list(mean=2, sd=0.4), aes(colour='mean=2, sd=0.4')) + 
  stat_function(fun=pnorm, args=list(mean=3, sd=0.3), aes(colour='mean=3, sd=0.3')) + 
  scale_color_manual('params', values = c('green', 'red', 'blue')) +
  labs(x="x", y="CDF")
```
This figure shows CDFs for normal distributions with a range of parameters. The sigmoid shape of these curves is a recognizable characteristic of a normal distribution.

We earlier looked at the distribution of birth weights. Next figure shows the empirical CDF of weights for all live births and the CDF of a normal distribution with the same mean and variance.
```{r}
live$totalwgt_lb %>%
  mean(na.rm=TRUE)
live$totalwgt_lb %>%
  sd(na.rm=TRUE)
```
```{r}
live %>%
  ggplot(aes(x=totalwgt_lb)) +
  stat_ecdf(geom="step", na.rm = TRUE, aes(color='data')) +
  stat_function(fun=pnorm, args=list(mean=7.28, sd=1.24), aes(color='model')) +
  labs(x="weight(pounds)", y="CDF") + 
  scale_color_manual('CDF', values = c('blue', 'red')) +
  theme(legend.position = c(0.8, 0.2)) 
```
The normal distribution is a good model for this dataset, so if we summarize the distribution with the parameters μ = 7.28 and σ = 1.24, the resulting error (difference between the model and the data) is small.

Below the 10th percentile there is a discrepancy between the data and the model; there are more light babies than we would expect in a normal distribution. If we are specifically interested in preterm babies, it would be important to get this part of the distribution right, so it might not be appropriate to use the normal model.

### Normal Probability Plot
For the exponential distribution, and a few others, there are simple transformations we can use to test whether an analytic distribution is a good model for a dataset.
For the normal distribution there is no such transformation, but there is an alternative called a normal probability plot.
```{r}
ggplot()+
  geom_qq(aes(sample = rnorm(400, mean = 0, sd = 1), color='mean = 0, sd = 1'), 
          geom='line') +
  geom_qq(aes(sample = rnorm(400, mean = 1, sd = 1), color='mean = 1, sd = 1'), 
          geom='line') +
  geom_qq(aes(sample = rnorm(400, mean = 5, sd = 2), color='mean = 5, sd = 2'), 
          geom='line') + 
  scale_color_manual('Parameters', values = c('blue', 'red', 'green')) +
  labs(x='standard normal sample', y='sample values') +
  theme(legend.position = c(0.2, 0.7)) 
```
*If the distribution of the sample is approximately normal, the result is a
straight line with intercept mu and slope sigma.* In the figure above, values are generated from normal distributions with different values of mu and sigma. The lines are approximately straight, with values in the tails deviating more than values near the mean.

Now let's try it with real data. We will generate the normal probability plot for the birth weight data.
```{r}
ggplot()+
  #taking all live births
  stat_qq(aes(sample=live$totalwgt_lb, color='all live'), 
          na.rm = TRUE, geom="line", distribution = qnorm) +
  #taking live births with full term only
  stat_qq(aes(sample=live$totalwgt_lb[live$prglngth>36], color='full term'), 
          na.rm = TRUE, geom="line", distribution = qnorm) +
  #the model as per mu and sigma
  geom_qq(aes(sample = rnorm(length(live$totalwgt_lb), mean = 7.4, sd = 1.2), 
              color='model'), geom='line') +
  scale_color_manual('Cases', values = c('blue', 'light blue', 'red')) +
  labs(x='Standard Deviations from mean', y='Birth weight(pounds)') +
  theme(legend.position = c(0.2, 0.7)) 
```
This figure shows the results for all live births, and also for full term births (pregnancy length greater than 36 weeks). Both curves match the model near the mean and deviate in the tails. The heaviest babies are heavier than what the model expects, and the lightest babies are lighter.
When we select only full term births, we remove some of the lightest weights, which reduces the discrepancy in the lower tail of the distribution. 
This plot suggests that the normal model describes the distribution well within a few standard deviations from the mean, but not in the tails. Whether it is good enough for practical purposes depends on the purposes.

### Lognormal distribution
If the logarithms of a set of values have a normal distribution, the values have a lognormal distribution. The CDF of the lognormal distribution is the same as the CDF of the normal distribution, with log x substituted for x.
The parameters of the lognormal distribution are usually denoted mu and sigma. But remember that these parameters are not the mean and standard deviation.

If a sample is approximately lognormal and you plot its CDF on a log-x scale, it will have the characteristic shape of a normal distribution. To test how well the sample fits a lognormal model, you can make a normal probability plot using the log of the values in the sample.
As an example, let's look at the distribution of adult weights, which is approximately lognormal. 
The National Center for Chronic Disease Prevention and Health Promotion conducts an annual survey as part of the Behavioral Risk Factor Surveillance System (BRFSS). In 2008, they interviewed 414,509 respondents and asked about their demographics, health, and health risks. Among the data they collected are the weights in kilograms of 398,484 respondents.
The codebook can be found at https://www.cdc.gov/brfss/annual_data/2008/pdf/codebook08.pdf
```{r}
brfss <- read.fwf('~/Documents/CodeWork/ThinkStats2/code/CDBRFS08.ASC.gz', 
                        widths = c(142, 1, 1107, 3, 5), 
                  col.names = c('v1', 'sex', 'v2', 'htm3', 'wtkg2'))
```

```{r}
#clean variables, as per the codebook
brfss$wtkg2[brfss$wtkg2==99999] <- NA
brfss$wtkg2 <- brfss$wtkg2/100.0

brfss$htm3[brfss$htm3==999] <- NA
```


```{r}
#calculate mean and standard deviations of the weights, to build the model
mu <- brfss$wtkg2 %>%
  mean(na.rm=TRUE)
sigma <- brfss$wtkg2 %>%
  sd(na.rm = TRUE)
#plot the theoretical normal distribution with calculated mean and standard deviation, against the empirical distribution
brfss %>%
  ggplot(aes(x=wtkg2)) +
  stat_ecdf(geom="step", na.rm = TRUE, aes(color='weights')) + 
  stat_function(fun=pnorm, args=list(mean=mu, sd=sigma), aes(color='model')) +
  labs(x="adult weight (kg)", y="CDF") + 
  scale_color_manual('CDF', values = c('blue', 'red')) +
  theme(legend.position = c(0.8, 0.2)) 
```

```{r}
#taking log of mean and standard deviation
logmu <- brfss$wtkg2 %>%
  log10 %>%
  mean(na.rm=TRUE)
logsigma <- brfss$wtkg2 %>%
  log10 %>%
  sd(na.rm=TRUE)

#plot the theoretical normal distribution with calculated log mean and log standard deviation, against the empirical log distribution
brfss %>%
  ggplot(aes(x=log10(wtkg2))) +
  stat_ecdf(geom="step", na.rm = TRUE, aes(color='weights')) + 
  stat_function(fun=pnorm, args=list(mean=logmu, sd=logsigma), aes(color='model')) +
  labs(x="adult weight (log10 kg)", y="CDF") + 
  scale_color_manual('CDF', values = c('blue', 'red')) +
  theme(legend.position = c(0.8, 0.2)) 
```
The two figures above show the distribution of adult weights on a linear scale with a normal model, and on a log scale with a lognormal model, respectively. The lognormal model is a better fit, but this representation of the data does not make the difference particularly dramatic.

The figures below shows normal probability plots for adult weights, w, and for their
logarithms, log10 w.
```{r}
ggplot()+
  stat_qq(aes(sample=brfss$wtkg2, color='weights'), 
          na.rm = TRUE, geom="line", distribution = qnorm) +
  #the model as per mu and sigma
  geom_qq(aes(sample = rnorm(length(brfss$wtkg2), mean=mu, sd=sigma), 
              color='model'), geom='line') +
  scale_color_manual('Cases', values = c('blue', 'red')) +
  labs(x='z', y='adult weight(kg)') +
  theme(legend.position = c(0.2, 0.7)) 
```

```{r}
ggplot()+
  #taking logarithmic weights
  stat_qq(aes(sample=log10(brfss$wtkg2), color='weights'), 
          na.rm = TRUE, geom="line", distribution = qnorm) +
  #the model as per mu and sigma
  geom_qq(aes(sample = rnorm(length(brfss$wtkg2), mean=logmu, sd=logsigma), 
              color='model'), geom='line') +
  scale_color_manual('Cases', values = c('blue', 'red')) +
  labs(x='z', y='adult weight(log10 kg)') +
  theme(legend.position = c(0.2, 0.7)) 
```
Now it is apparent that the data deviate substantially
from the normal model. On the other hand, the lognormal model is a good
match for the data.

### Pareto distribution
The Pareto distribution is named after the economist Vilfredo Pareto, who used it to describe the distribution of wealth. Since then, it has been used to describe phenomena in the natural and social sciences including sizes of cities and towns, sand particles and meteorites, forest fires and earthquakes.
The CDF of the Pareto distribution is:
CDF(x) = 1 - (x/xm)^-a
The parameters xm and a determine the location and shape of the distribution. xm is the minimum possible value. Figure below shows Pareto distributions for xm = 0.5 and different values of a.
```{r}
data.frame(x = c(0, 10)) %>%
  ggplot(mapping = aes(x)) +
  stat_function(fun=ppareto, args=list(a=0.5, b=0.5), aes(colour='a=0.5')) + 
  stat_function(fun=ppareto, args=list(a=1, b=0.5), aes(colour='a=1')) + 
  stat_function(fun=ppareto, args=list(a=2, b=0.5), aes(colour='a=2')) + 
  scale_color_manual('params', values = c('green', 'red', 'blue')) +
  labs(x="x", y="CDF")
```
There is a simple visual test that indicates whether an empirical distribution fits a Pareto distribution: on a log-log scale, the CCDF looks like a straight line.
As an example, let's look at the sizes of cities and towns. The U.S. Census Bureau publishes the population of every incorporated city and town in the United States.
```{r}
#reading data
df <- read.csv('~/Documents/CodeWork/ThinkStats2/code/PEP_2012_PEPANNRES_with_ann.csv', stringsAsFactors = FALSE, skip = 1)

#extract population
pops <- df[7] %>% 
  rapply(function(x) ifelse(x==0,NA,x), how = "replace") %>%
  drop_na

#get log10 of population
log_pops <- log10(pops)
```


```{r}
ggplot(data.frame(x=log_pops), aes(x)) +
  geom_step(aes(y=log10(1-..y..)), stat='ecdf', na.rm=TRUE, color='blue') +
  geom_step(data=data.frame(x=rpareto(length(pops), a=1.4, b=5000)), 
            aes(x=log10(x), y=log10(1-..y..)), stat='ecdf', color='red')
```

This figure shows the CCDF of populations on a log-log scale. The largest 1% of cities and towns, below 10^-2, fall along a straight line. So we could conclude, as some researchers have, that the tail of this distribution fits a Pareto model.

On the other hand, a lognormal distribution also models the data well. 

```{r}
#taking log of mean and standard deviation
logmu <- log_pops %>%
  mean(na.rm=TRUE)
logsigma <- log_pops %>%
  sd(na.rm=TRUE)

#plot the theoretical normal distribution with calculated log mean and log standard deviation, against the empirical log distribution
data.frame(x=log_pops) %>%
  ggplot(aes(x=x)) +
  stat_ecdf(geom="step", na.rm = TRUE, aes(color='data')) + 
  stat_function(fun=pnorm, args=list(mean=logmu, sd=logsigma), aes(color='model')) +
  labs(x="log10 population", y="CDF") + 
  scale_color_manual('CDF', values = c('blue', 'red')) +
  theme(legend.position = c(0.8, 0.2)) 
```

```{r}
mu <- log_pops %>%
  mean(na.rm=TRUE)

sigma <- log_pops %>%
  sd(na.rm=TRUE)

ggplot()+
  stat_qq(aes(sample=log_pops, color='data'), 
          na.rm = TRUE, geom="line", distribution = qnorm) +
  #the model as per mu and sigma
  geom_qq(aes(sample = rnorm(length(log_pops), mean = mu, sd = sigma), 
              color='model'), geom='line') +
  scale_color_manual('Cases', values = c('blue', 'red')) +
  labs(x='Standard Deviations from mean (z)', y='log10 population') +
  theme(legend.position = c(0.2, 0.7)) 
```

The figures above show the CDF of populations and a lognormal model (top), and a normal probability plot (right). Both plots show good agreement between the data and the model.
Neither model is perfect. The Pareto model only applies to the largest 1% of cities, but it is a better fit for that part of the distribution. The lognormal model is a better fit for the other 99%. Which model is appropriate depends on which part of the distribution is relevant.

### Generating Random Numbers
Analytic CDFs can be used to generate random numbers with a given distribution function, p = CDF(x). If there is an effcient way to compute the inverse CDF, we can generate random values with the appropriate distribution by choosing p from a uniform distribution between 0 and 1, then choosing x = ICDF(p).

### Why model?
Like all models, analytic distributions are abstractions, which means they leave out details that are considered irrelevant. For example, an observed distribution might have measurement errors or quirks that are specific to the sample; analytic models smooth out these idiosyncrasies.
Analytic models are also a form of data compression. When a model fits a dataset well, a small set of parameters can summarize a large amount of data.
It is sometimes surprising when data from a natural phenomenon fit an analytic distribution, but these observations can provide insight into physical systems. Sometimes we can explain why an observed distribution has a particular form. For example, Pareto distributions are often the result of generative processes with positive feedback (so-called preferential attachment processes).
Also, analytic distributions lend themselves to mathematical analysis.
But it is important to remember that all models are imperfect. Data from the real world never fit an analytic distribution perfectly. People sometimes talk as if data are generated by models; for example, they might say that the distribution of human heights is normal, or the distribution of income is lognormal. Taken literally, these claims cannot be true; there are always differences between the real world and mathematical models.


## Probability Density Functions
### PDFs
The derivative of a CDF is called a probability density function, or PDF. For example, the PDF of an exponential distribution is:
PDFexpo(x) = λe^(-λx)

Evaluating a PDF for a particular value of x is usually not useful. The result
is not a probability; it is a probability density.
In physics, density is mass per unit of volume; in order to get a mass, you have to multiply by volume or, if the density is not constant, you have to integrate over volume.
Similarly, probability density measures probability per unit of x. In order to get a probability mass, you have to integrate over x.

The following example creates a NormalPdf with the mean and variance of adult female heights, in cm, from the BRFSS.
```{r}
#filtering data for female heights
adult_female_heights <- brfss %>%
  subset(sex==2) %>%
  .$htm3
mean <- adult_female_heights %>%
  mean(na.rm=TRUE)
sd <- adult_female_heights %>%
  sd(na.rm=TRUE)
#random generation for the normal distibution with mean and standard deviation equal to thos of female heights
brfss_norm <- adult_female_heights %>%
  rnorm(mean=mean, sd=sd)
```

```{r}
brfss_norm %>%
  data.frame(x=.) %>%
  ggplot(aes(x=x, ..density..)) + 
  geom_freqpoly(binwidth = 2, na.rm = TRUE, aes(color='normal'))+
  geom_density(n=500, na.rm = TRUE, aes(color='sample KDE'), kernel='gaussian')+
  scale_color_manual('Cases', values = c('blue', 'red')) +
  labs(x="Height(cm)", y="density") +
  theme(legend.position = c(0.2, 0.7))
```

### Kernel Density Estimation
Kernel density estimation (KDE) is an algorithm that takes a sample and finds an appropriately smooth PDF that fits the data.
Figure above shows the normal density function that models adult female heights in the  USand a KDE based on a sample of 500 random heights. The estimate is a good match for the original distribution. The word "gaussian" appears in the kernel argument in geom_density() function because it uses a filter based on a Gaussian distribution to smooth the KDE.

Estimating a density function with KDE is useful for several purposes:
* Visualization: During the exploration phase of a project, CDFs are usually the best visualization of a distribution. After you look at a CDF, you can decide whether an estimated PDF is an appropriate model of the distribution. If so, it can be a better choice for presenting the distribution to an audience that is unfamiliar with CDFs.
* Interpolation: An estimated PDF is a way to get from a sample to a model of the population. If you have reason to believe that the population distribution is smooth, you can use KDE to interpolate the density for values that don't appear in the sample.
* Simulation: Simulations are often based on the distribution of a sample. If the sample size is small, it might be appropriate to smooth the sample distribution using KDE, which allows the simulation to explore more possible outcomes, rather than replicating the observed.

### The distribution framework
At this point we have seen PMFs, CDFs and PDFs; let's take a minute to review. 
We started with PMFs, which represent the probabilities for a discrete set of values. To get from a PMF to a CDF, you add up the probability masses to get cumulative probabilities. To get from a CDF back to a PMF, you compute differences in cumulative probabilities. We'll see the implementation of these operations in the next few sections.
A PDF is the derivative of a continuous CDF; or, equivalently, a CDF is the integral of a PDF. Remember that a PDF maps from values to probability densities; to get a probability, you have to integrate.
To get from a discrete to a continuous distribution, you can perform various kinds of smoothing. One form of smoothing is to assume that the data come from an analytic continuous distribution (like exponential or normal) and to estimate the parameters of that distribution. Another option is kernel density estimation.
The opposite of smoothing is discretizing, or quantizing. If you evaluate a PDF at discrete points, you can generate a PMF that is an approximation of the PDF. You can get a better approximation using numerical integration. To distinguish between continuous and discrete CDFs, it might be better for a discrete CDF to be a "cumulative mass function," but as far as I can tell no one uses that term.

### Moments
Any time you take a sample and reduce it to a single number, that number is a statistic. The statistics we have seen so far include mean, variance, median, and interquartile range.
A raw moment is a kind of statistic. If you have a sample of values, x(i), the kth raw moment is:
m(k) = 1/n*sum(x(i)^k)
When k = 1 the result is the sample mean. The other raw moments don't mean much by themselves, but they are used in some computations.
The central moments are more useful. The kth central moment is:
m(k) = 1/n*sum((x(i) - mean(x))^k)
When k = 2 the result is the second central moment, which you might recognize as variance. 
When you report moment-based statistics, it is important to think about the units. For example, if the values xi are in cm, the first raw moment is also in cm. But the second moment is in cm square, the third moment is in cm cube, and so on.
Because of these units, moments are hard to interpret by themselves. That's why, for the second moment, it is common to report standard deviation, which is the square root of variance, so it is in the same units as x(i).

### Skewness
Skewness is a property that describes the shape of a distribution. If the distribution is symmetric around its central tendency, it is unskewed. If the values extend farther to the right, it is "right skewed" and if the values extend left, it is "left skewed."
Given a sequence of values, xi, the sample skewness, g1, can be computed like this:

```{r}
xi <- rexp(100)
g1 <- xi %>%
  timeDate::skewness()
g1
```

Negative skewness indicates that a distribution skews left; positive skewness indicates that a distribution skews right. The magnitude of g1 indicates the strength of the skewness, but by itself it is not easy to interpret.
In practice, computing sample skewness is usually not a good idea. If there are any outliers, they have a disproportionate effect on g1.
Another way to evaluate the asymmetry of a distribution is to look at the relationship between the mean and median. Extreme values have more effect on the mean than the median, so in a distribution that skews left, the mean is less than the median. In a distribution that skews right, the mean is greater.

Pearson's median skewness coeffcient is a measure of skewness based on the difference between the sample mean and median:
g(p) = 3*(mean - median)/sd
This statistic is robust, which means that it is less vulnerable to the effect of outliers. 
As an example, let's look at the skewness of birth weights in the NSFG pregnancy data.

```{r}
#calculate sample skewness
xi <- live %>%
  subset(!is.na(totalwgt_lb)) %>%
  .$totalwgt_lb
g1 <- xi %>%
  timeDate::skewness()
g1
```
```{r}
#calculate Pearson's median skewness coefficient
m <- live$totalwgt_lb %>%
  mean(na.rm=TRUE)
md <- live$totalwgt_lb %>%
  median(na.rm=TRUE)
sigma <- live$totalwgt_lb %>%
  sd(na.rm=TRUE)
gp <- 3*(m-md)/sigma
gp
```


```{r}
live %>%
  ggplot(aes(x=totalwgt_lb)) + 
  geom_density(na.rm = TRUE, color='blue', kernel='gaussian') +
  geom_vline(aes(xintercept = mean(x=totalwgt_lb, na.rm=TRUE)), 
             colour='red') +
  annotate("text", x = mean(x=live$totalwgt_lb, na.rm=TRUE)-0.3, y = 0.1, 
           label = "mean", color = 'red', angle=90) +
  geom_vline(aes(xintercept = median(x=totalwgt_lb, na.rm=TRUE)), 
             colour='black') +
  annotate("text", x = median(x=live$totalwgt_lb, na.rm=TRUE)+0.3, y = 0.1, 
           label = "median", color = 'black', angle=90) +
  labs(x="lbs", y="PDF")
```

The left tail appears longer than the right, so we suspect the distribution is skewed left. The mean, 7.27 lbs, is a bit less than the median, 7.38 lbs, so that is consistent with left skew. And
both skewness coefficients are negative: sample skewness is -0.59; Pearson's median skewness is -0.23.

Now let's compare this distribution to the distribution of adult weight in the BRFSS.

```{r}
#calculate sample skewness
xi <- brfss %>%
  subset(!is.na(wtkg2)) %>%
  .$wtkg2
g1 <- xi %>%
  timeDate::skewness()
g1
```

```{r}
#calculate Pearson's median skewness coefficient
m <- brfss$wtkg2 %>%
  mean(na.rm=TRUE)
md <- brfss$wtkg2 %>%
  median(na.rm=TRUE)
sigma <- brfss$wtkg2 %>%
  sd(na.rm=TRUE)
gp <- 3*(m-md)/sigma
gp
```

```{r}
mean <- brfss$wtkg2 %>%
  mean(na.rm=TRUE)
sd <- brfss$wtkg2 %>%
  sd(na.rm=TRUE)
#random generation for the normal distibution with mean and standard deviation equal to those of adult heights
brfss_norm <- brfss$wtkg2 %>%
  rnorm(mean=mean, sd=sd)
```

```{r}
brfss %>%
  ggplot(aes(x=wtkg2)) + 
  geom_density(adjust = 1.5, na.rm = TRUE, color='blue', kernel='gaussian') +
  geom_vline(aes(xintercept = mean(x=brfss$wtkg2, na.rm=TRUE)), 
             colour='red') +
  annotate("text", x = mean(x=brfss$wtkg2, na.rm=TRUE) + 4, y = 0.01, 
           label = "mean", color = 'red', angle=90) +
  geom_vline(aes(xintercept = median(x=brfss$wtkg2, na.rm=TRUE)), 
             colour='black') +
  annotate("text", x = median(x=brfss$wtkg2, na.rm=TRUE) - 4, y = 0.01, 
           label = "median", color = 'black', angle=90) +
  labs(x="kg", y="PDF") 
```

The distribution appears skewed to the right.
Sure enough, the mean, 79.0, is bigger than the median, 77.3. The sample skewness is 1.1 and Pearson's median skewness is 0.26.
The sign of the skewness coefficient indicates whether the distribution skews left or right, but other than that, they are hard to interpret. Sample skewness is less robust; that is, it is more susceptible to outliers. As a result it is less reliable when applied to skewed distributions, exactly when it would be most relevant.
Pearson's median skewness is based on a computed mean and variance, so it is also susceptible to outliers, but since it does not depend on a third moment, it is somewhat more robust.

## Relationships between variables
### Scatter Plots
The simplest way to check for a relationship between two variables is a scatter plot, but making a good scatter plot is not always easy. As an example, we'll plot weight versus height for the respondents in the BRFSS. People who are taller tend to be heavier. Of course, it is not a perfect relationship: there are short heavy people and tall light ones. But if you are trying to guess someone's weight, you will be more accurate if you know their height than if you don't. 
We'll be sampling 2000 rows, without replacement. 

```{r}
brfss %>%
  sample_n(2000) %>%
  ggplot() +
  geom_point(aes(x=htm3, y=wtkg2), na.rm=TRUE, color='blue') +
  xlim(140, 210) +
  ylim(20, 200) +
  labs(x='height (cm)', y='weight (kg)')
```

As we expected, taller people tend to be heavier.
But this is not the best representation of the data, because the data are packed into columns. The problem is that the heights are rounded to the nearest inch, converted to centimeters, and then rounded again. Some information is lost in translation.
We can't get that information back, but we can minimize the effect on the scatter plot by jittering the data, which means adding random noise to reverse the effect of rounding off. Since these measurements were rounded to the nearest inch, they might be off by up to 0.5 inches or 1.3 cm. Similarly, the weights might be off by 0.5 kg.

```{r}
brfss %>%
  sample_n(2000) %>%
  ggplot() +
  geom_jitter(aes(x=htm3, y=wtkg2), na.rm=TRUE, width = 1.3, height = 0.5, color='blue') +
  xlim(140, 210) +
  ylim(20, 200) +
  labs(x='height (cm)', y='weight (kg)')
```

Jittering reduces the visual effect of rounding and makes the shape of the relationship clearer. But in general you should only jitter data for purposes of visualization and avoid using jittered
data for analysis.
Even with jittering, this is not the best way to represent the data. There are many overlapping points, which hides data in the dense parts of the figure and gives disproportionate emphasis to outliers. This effect is called saturation.
We can solve this problem with the alpha parameter, which makes the points partly transparent:

```{r}
brfss %>%
  sample_n(2000) %>%
  ggplot() +
  geom_jitter(aes(x=htm3, y=wtkg2), na.rm=TRUE, 
              width = 1.3, height = 0.5, color='blue', alpha=0.2) +
  xlim(140, 210) +
  ylim(20, 200) +
  labs(x='height (cm)', y='weight (kg)')
```

This figure shows the result. Overlapping data points look darker, so darkness is proportional to density. In this version of the plot, we can see two details that were not apparent before: vertical clusters at several heights and a horizontal line near 90 kg or 200 pounds. Since this data is based on self-reports in pounds, the most likely explanation is that some respondents reported rounded values.

Using transparency works well for moderate-sized datasets, but this figure only shows the first 5000 records in the BRFSS, out of a total of 414 509.
To handle larger datasets, another option is a hexbin plot, which divides the
graph into hexagonal bins and colors each bin according to how many data
points fall in it.

```{r}
brfss %>%
  ggplot() +
  geom_hex(aes(x=htm3, y=wtkg2), na.rm = TRUE) +
  xlim(140, 210) +
  ylim(20, 200) +
  labs(x='height (cm)', y='weight (kg)')
```

The figure above shows the result. An advantage of a hexbin is that it shows the shape of the relationship well, and it is efficient for large datasets. A drawback is that it makes the outliers invisible.
The point of this example is that it is not easy to make a scatter plot that shows relationships clearly without introducing misleading artifacts.

### Characterizing relationships
# HOLDING THIS PART FOR NOW
Scatter plots provide a general impression of the relationship between variables, but there are other visualizations that provide more insight into the nature of the relationship. One option is to bin one variable and plot percentiles of the other.
```{r}
bins = seq(135, 210, 5)
bins <- bins %>%
  append(0, after = 0)
bins <- bins %>% 
  append(max(brfss$htm3, na.rm = TRUE))
indices <- brfss %>%
  subset(!is.na(htm3)) %>%
  .$htm3 %>%
  cut(length(bins), breaks = bins, include.lowest = TRUE, labels = FALSE) %>%
  as.numeric()
```

### Correlation
A correlation is a statistic intended to quantify the strength of the relationship between two variables.
A challenge in measuring correlation is that the variables we want to compare are often not expressed in the same units. And even if they are in the same units, they come from different distributions.

There are two common solutions to these problems:
1. Transform each value to a standard scores, which is the number of standard deviations from the mean. This transform leads to the "Pearson product-moment correlation coefficient."
2. Transform each value to its rank, which is its index in the sorted list of values. This transform leads to the "Spearman rank correlation coeffcient."

If X is a series of n values, xi, we can convert to standard scores by subtracting the mean and dividing by the standard deviation: zi = (xi - mu)/sigma.

The numerator is a deviation: the distance from the mean. Dividing by sigma standardizes the deviation, so the values of Z are dimensionless (no units) and their distribution has mean 0 and variance 1.

If X is normally distributed, so is Z. But if X is skewed or has outliers, so does Z; in those cases, it is more robust to use percentile ranks. If we compute a new variable, R, so that ri is the rank of xi, the distribution of R is uniform from 1 to n, regardless of the distribution of X.

### Covariance
Covariance is a measure of the tendency of two variables to vary together.
If we have two series, X and Y , their deviations from the mean are
  dxi = xi - mean(x)
  dyi = yi - mean(y)
where mean(x) is the sample mean of X and mean(y) is the sample mean of Y . If X and Y vary together, their deviations tend to have the same sign.
If we multiply them together, the product is positive when the deviations have the same sign and negative when they have the opposite sign. So adding up the products gives a measure of the tendency to vary together.
Covariance is the mean of these products:
Cov(X, Y ) = 1/n * sum(dxi * dyi)
where n is the length of the two series (they have to be the same length).
If you have studied linear algebra, you might recognize that Cov is the dot product of the deviations, divided by their length. So the covariance is maximized if the two vectors are identical, 0 if they are orthogonal, and negative if they point in opposite directions.

### Pearson's correlation
Covariance is useful in some computations, but it is seldom reported as a summary statistic because it is hard to interpret. Among other problems, its units are the product of the units of X and Y . For example, the covariance of weight and height in the BRFSS dataset is 113 kilogram-centimeters, whatever that means.
One solution to this problem is to divide the deviations by the standard deviation, which yields standard scores, and compute the product of standard scores.
This value is called Pearson's correlation(ρ) after Karl Pearson, an influential early statistician. It is easy to compute and easy to interpret. Because standard scores are dimensionless, so is ρ.

Pearson's correlation is always between -1 and +1 (including both). If ρ is positive, we say that the correlation is positive, which means that when one variable is high, the other tends to be high. If ρ is negative, the correlation is negative, so when one variable is high, the other is low.
The magnitude of ρ indicates the strength of the correlation. If ρ is 1 or -1, the variables are perfectly correlated, which means that if you know one, you can make a perfect prediction about the other.
Most correlation in the real world is not perfect, but it is still useful. The correlation of height and weight is 0.51, which is a strong correlation compared to similar human-related variables.
```{r}
brfss$htm3 %>%
  #use = "complete.obs ignores the NAs"
  cor(brfss$wtkg2, use="complete.obs", method = "pearson")
```

### Non-linear relationships
If Pearson's correlation is near 0, it is tempting to conclude that there is no relationship between the variables, but that conclusion is not valid. Pearson's correlation only measures linear relationships. If there's a nonlinear relationship, ρ understates its strength.
You should always look at a scatter plot of your data before blindly computing a correlation coeffcient.

### Spearman's rank correlation
Pearson's correlation works well if the relationship between variables is linear and if the variables are roughly normal. But it is not robust in the presence of outliers. Spearman's rank correlation is an alternative that mitigates the effect of outliers and skewed distributions. To compute Spearman's correlation, we have to compute the rank of each value, which is its index in the sorted sample. For example, in the sample [1, 2, 5, 7] the rank of the value 5 is 3, because it appears third in the sorted list. Then we compute Pearson's correlation for the ranks.
```{r}
brfss$htm3 %>%
  #use = "complete.obs ignores the NAs"
  cor(brfss$wtkg2, use="complete.obs", method = "spearman")
```

The Spearman rank correlation for the BRFSS data is 0.54, a little higher than the Pearson correlation, 0.51. There are several possible reasons for the difference, including:
1. If the relationship is nonlinear, Pearson's correlation tends to underestimate the strength of the relationship, and
2. Pearson's correlation can be affected (in either direction) if one of the distributions is skewed or contains outliers. Spearman's rank correlation is more robust.
In the BRFSS example, we know that the distribution of weights is roughly lognormal; under a log transform it approximates a normal distribution, so it has no skew. So another way to eliminate the effect of skewness is to compute Pearson's correlation with log-weight and height:
```{r}
log10(brfss$wtkg2) %>%
  #use = "complete.obs ignores the NAs"
  cor(brfss$htm3, use="complete.obs", method = "pearson")
```

The result is 0.53, close to the rank correlation, 0.54. So that suggests that skewness in the distribution of weight explains most of the difference between Pearson's and Spearman's correlation.

### Correlation and causation
If variables A and B are correlated, there are three possible explanations: A causes B, or B causes A, or some other set of factors causes both A and B. These explanations are called "causal relationships".
Correlation alone does not distinguish between these explanations, so it does not tell you which ones are true. This rule is often summarized with the phrase "Correlation does not imply causation".
So what can you do to provide evidence of causation?
1. Use time. If A comes before B, then A can cause B but not the other way around (at least according to our common understanding of causation). The order of events can help us infer the direction of causation, but it does not preclude the possibility that something else causes both A and B.
2. Use randomness. If you divide a large sample into two groups at random and compute the means of almost any variable, you expect the difference to be small. If the groups are nearly identical in all variables but one, you can eliminate spurious relationships.
This works even if you don't know what the relevant variables are, but it works even better if you do, because you can check that the groups are identical.
These ideas are the motivation for the randomized controlled trial, in which subjects are assigned randomly to two (or more) groups: a treatment group that receives some kind of intervention, like a new medicine, and a control group that receives no intervention, or another treatment whose effects are known.
A randomized controlled trial is the most reliable way to demonstrate a causal relationship, and the foundation of science-based medicine (see http://wikipedia.org/wiki/Randomized_controlled_trial).
In some cases it is possible to infer causal relationships using regression analysis, which will be discussed later.

## Estimation
### The estimaton game
Let's play a game. I think of a distribution, and you have to guess what it is. I'll give you two hints: it's a normal distribution, and here's a random sample drawn from it:
[-0.441, 1.774, -0.101, -1.138, 2.975, -2.138]
What do you think is the mean parameter, μ, of this distribution?
One choice is to use the sample mean, x̄, as an estimate of μ. In this example, x̄ is 0.155, so it would be reasonable to guess μ = 0.155. This process is called estimation, and the statistic we used (the sample mean) is called an estimator.
Using the sample mean to estimate μ is so obvious that it is hard to imagine a reasonable alternative. But suppose we change the game by introducing outliers.
I'm thinking of a distribution. It's a normal distribution, and here's a sample that was collected by an unreliable surveyor who occasionally puts the decimal point in the wrong place.
[-0.441, 1.774, -0.101, -1.138, 2.975, -213.8]

Now what's your estimate of μ? If you use the sample mean, your guess is -35.12. Is that the best choice? What are the alternatives?
One option is to identify and discard outliers, then compute the sample mean of the rest. Another option is to use the median as an estimator.
Which estimator is best depends on the circumstances (for example, whether there are outliers) and on what the goal is. Are you trying to minimize errors, or maximize your chance of getting the right answer?
If there are no outliers, the sample mean minimizes the mean squared error (MSE). That is, if we play the game many times, and each time compute the
error x̄-μ, the sample mean minimizes:
MSE = 1/m * (mean(x)-mu)^2
Where m is the number of times you play the estimation game, not to be confused with n, which is the size of the sample used to compute mean(x).

```{r}
#function to calculate RMSE given the estimates and actuals
rmse <- function(estimates, actual) {
  error = estimates - actual
  error^2 %>%
    mean %>%
    sqrt 
  }
```
estimates is a list of estimates; actual is the actual value being estimated. In practice, of course, we don't know actual; if we did, we wouldn't have to estimate it. The purpose of this experiment is to compare the performance of the two estimators.

Here is a function that simulates the estimation game and computes the root mean squared error (RMSE), which is the square root of MSE:
```{r}
estimate <- function(n=7, m=1000){
  mu = 0
  sigma = 1
  
  means = c()
  medians = c()
  
  for (i in seq(1,m)){
    xs = rnorm(n, mu, sigma)
    xbar = mean(xs)
    md = median(xs)
    means <- means %>%
      append(xbar)
    medians <- medians %>%
      append(md)
  }
  
  'rmse xbar' %>%
    cat(rmse(means, mu), "\n")
  'rmse median' %>%
    cat(rmse(medians, mu))
}
estimate()
```

Again, n is the size of the sample, and m is the number of times we play the game. "means" is the list of estimates based on xbar. "medians" is the list of medians.

When we run this code, the RMSE of the sample mean is 0.37, which means that if we use xbar to estimate the mean of this distribution, based on a sample with n = 7, we should expect to be off by 0.37 on average. Using the median to estimate the mean yields RMSE 0.53, which confirms that xbar yields lower RMSE, at least for this example.

Minimizing MSE is a nice property, but it's not always the best strategy. For example, suppose we are estimating the distribution of wind speeds at a building site. If the estimate is too high, we might overbuild the structure, increasing its cost. But if it's too low, the building might collapse. Because cost as a function of error is not symmetric, minimizing MSE is not the best strategy.

As another example, suppose I roll three six-sided dice and ask you to predict the total. If you get it exactly right, you get a prize; otherwise you get nothing. In this case the value that minimizes MSE is 10.5, but that would be a bad guess, because the total of three dice is never 10.5. For this game, you want an estimator that has the highest chance of being right, which is a maximum likelihood estimator (MLE). If you pick 10 or 11, your chance of winning is 1 in 8, and that's the best you can do.

### Guess the variation
I'm thinking of a distribution. It's a normal distribution, and here's a (familiar) sample:
[-0.441, 1.774, -0.101, -1.138, 2.975, -2.138]
What do you think is the variance, sigma^2, of my distribution? Again, the obvious choice is to use the sample variance, S^2, as an estimator.
For large samples, S^2 is an adequate estimator, but for small samples it tends to be too low. Because of this unfortunate property, it is called a biased estimator. An estimator is unbiased if the expected total (or mean) error, after many iterations of the estimation game, is 0.
Here is a function that simulates the estimation game and tests the performance of S^2 and S^2(n-1):
�1:
```{r}
#function to calculate mean error from estimates and actual
meanError <- function(estimates, actual){
  errors <- estimates - actual
  mean(errors)
}
```
MeanError computes the mean difference between the estimates and the actual value.

```{r}
estimate <- function(n=7, m=1000){
  mu = 0
  sigma = 1
  
  estimates1 = c()
  estimates2 = c()
  
  for (i in seq(1,m)){
    xs = rnorm(n, mu, sigma)
    #var gives sample(unbiased) variance, so modifying for biased variance
    biased = var(xs)*(n-1)/n
    unbiased = var(xs)
    estimates1 <- append(estimates1, biased)
    estimates2 <- append(estimates2, unbiased)
  }
  
  'mean error biased' %>%
    cat(meanError(estimates1, sigma^2), "\n")
  'mean error unbiased' %>%
    cat(meanError(estimates2, sigma^2))
}
estimate()
```
Again, n is the sample size and m is the number of times we play the game. var computes S^2(n-1) by default.

The mean error for S^2 is -0.14. As expected, this biased estimator tends to be too low. For S^2(n-1), the mean error was 0.008, about 10 times smaller. As m increases, we expect the mean error for S^2(n-1) to approach 0.

