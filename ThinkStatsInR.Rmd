---
title: "thinkstats"
output: html_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(ggplot2)
library(tidyverse)
library(plotly)
```

The data source is obtained from https://www.cdc.gov/nchs/nsfg/nsfg_cycle6.htm
This contains fixed width files, and stata dictionaries consisting of columns for the data files
```{r}
#helper function to parse Stata dictionary
dct.parser <- function(dct, includes = c("StartPos", "StorageType", "ColName", 
                                         "ColWidth", "VarLabel"),
                       preview = FALSE) {
  temp <- readLines(dct)
  temp <- temp[grepl("_column", temp)]
  
  if (isTRUE(preview)) {
    head(temp)
  } else {
    possibilities <- c("StartPos", "StorageType", 
                       "ColName", "ColWidth", "VarLabel")
    classes <- c("numeric", "character", "
                 character", "numeric", "character")
    pattern <- c(StartPos = ".*\\(([0-9 ]+)\\)",
                 StorageType = "(byte|int|long|float|double|str[0-9]+)",
                 ColName = "(.*)",
                 ColWidth = "%([0-9.]+)[a-z]+",
                 VarLabel = "(.*)")
    
    mymatch <- match(includes, possibilities)
    
    pattern <- paste(paste(pattern[mymatch], 
                           collapse ="\\s+"), "$", sep = "")    
    
    metadata <- setNames(lapply(seq_along(mymatch), function(x) {
      out <- gsub(pattern, paste("\\", x, sep = ""), temp)
      out <- gsub("^\\s+|\\s+$", "", out)
      out <- gsub('\"', "", out, fixed = TRUE)
      class(out) <- classes[mymatch][x] ; out }), 
                         possibilities[mymatch])
    
    implicit.dec <- grepl("\\.[1-9]", metadata[["ColWidth"]])
    if (any(implicit.dec)) {
      message("Some variables may need to be corrected for implicit decimals. 
              Try 'MESSAGES(output_from_dct.parser)' for more details.")
      metadata[["Decimals"]] <- rep(NA, length(metadata[["ColWidth"]]))
      metadata[["Decimals"]][implicit.dec] <-
        as.numeric(gsub("[0-9]+\\.", "", 
                        metadata[["ColWidth"]][implicit.dec]))
      metadata[["ColWidth"]] <- floor(as.numeric(metadata[["ColWidth"]]))
    }
    
    metadata[["ColName"]] <- make.names(
      gsub("\\s", "", metadata[["ColName"]]))
    
    metadata <- data.frame(metadata)
    
    if ("StorageType" %in% includes) {
      metadata <- 
        within(metadata, {
          colClasses <- ifelse(
            StorageType == "byte", "raw",
            ifelse(StorageType %in% c("double", "long", "float"), 
                   "numeric", 
                   ifelse(StorageType == "int", "integer",
                          ifelse(substr(StorageType, 1, 3) == "str", 
                                 "character", NA))))
        })
    }
    if (any(implicit.dec)) {
      attr(metadata, "MESSAGE") <- c(sprintf("%s", paste(
        "Some variables might need to be corrected for implicit decimals. 
        A variable, 'Decimals', has been created in the metadata that
        indicates the number of decimal places the variable should hold. 
        To correct the output, try (where your stored output is 'mydf'): 
        
        lapply(seq_along(mydf[!is.na(Decimals)]), 
        function(x) mydf[!is.na(Decimals)][x]
        / 10^Decimals[!is.na(Decimals)][x])
        
        The variables in question are:
        ")), sprintf("%s", metadata[["ColName"]][!is.na(metadata[["Decimals"]])]))
            class(attr(metadata, "MESSAGE")) <- c(
                "MESSAGE", class(attr(metadata, "MESSAGE")))
        }
        attr(metadata, "original.dictionary") <- 
            c(dct, basename(dct))
        metadata
    }
}
```


We can read the coulmns from 2002FemPreg.dct and use those columns to import the data from the fixed width file 2002FemPreg.dat
```{r}
femPreg2002columns <- dct.parser('~/Documents/CodeWork/ThinkStats2/code/2002FemPreg.dct')
femPreg2002 <- read.fwf('~/Documents/CodeWork/ThinkStats2/code/2002FemPreg.dat', widths = femPreg2002columns$ColWidth, col.names = femPreg2002columns$ColName)
```

Taking a look at the data
```{r}
head(femPreg2002)
```

We can see a lot of missing values. We'll clean the data for the columns that we want to analyze.

## Transformation

1. agepreg contains the mother's age at the end of the pregnancy. In the data file, agepreg is encoded as an integer number of centiyears. So the first line divides each element of agepreg by 100, yielding a floating-point value in years.

2. birthwgt_lb and birthwgt_oz contain the weight of the baby, in pounds and ounces, for pregnancies that end in live birth. In addition it uses several special codes:
  97 NOT ASCERTAINED
  98 REFUSED
  99 DONT KNOW
Special values encoded as numbers are dangerous because if they are not
handled properly, they can generate bogus results, like a 99-pound baby. Assuming that a baby can't be generally more than 20 lb at birth, we will replace all other values with NA, as they are NOT ASCERTAINED(97),  REFUSED(98), DONT KNOW(99), or invalid values.
Similarly, the age of father has these similar special codes, which we will replace by NA
```{r}
cleanFemPreg <- function(data){
  # mother's age is encoded in centiyears; convert to years
  data['agepreg'] <-  data['agepreg']/100.0
  
  # birthwgt_lb contains at least one bogus value (51 lbs)
  # replace with NaN
  data$birthwgt_lb[data$birthwgt_lb > 20] <- NA
  
  # replace 'not ascertained', 'refused', 'don't know' with NA
  na_vals = c(97, 98, 99)
  data$birthwgt_oz[data$birthwgt_oz %in% na_vals] <- NA
  data$hpagelb[data$hpagelb %in% na_vals] <- NA
  
  # birthweight is stored in two columns, lbs and oz.
  # convert to a single column in lb
  data['totalwgt_lb'] <- data$birthwgt_lb + (data$birthwgt_oz / 16.0)
  
  return (data)
}
```

```{r}
femPregCleaned <- cleanFemPreg(femPreg2002)
```

### Validation
One way to validate data is to compute basic statistics and compare them with published results. For example, the NSFG codebook includes tables that summarize each variable. Here is the table for outcome, which encodes the outcome of each pregnancy:
value     label         Total
1         LIVE BIRTH        9148
2         INDUCED ABORTION  1862
3         STILLBIRTH        120
4         MISCARRIAGE       1921
5         ECTOPIC PREGNANCY 190
6         CURRENT PREGNANCY 352

```{r}
femPreg2002 %>%
  group_by(outcome) %>%
  summarise(Total = length(outcome))
```

Comparing the results with the published table, it looks like the values in
outcome are correct. Similarly, here is the published table for birthwgt_lb
value     label             Total
.         INAPPLICABLE      4449
0-5       UNDER 6 POUNDS    1125
6         6 POUNDS          2223
7         7 POUNDS          3049
8         8 POUNDS          1889
9-95      9 POUNDS OR MORE  799

```{r}
femPreg2002 %>%
  group_by(birthwgt_lb) %>%
  summarise(Total = length(birthwgt_lb))
```

The counts for 6, 7, and 8 pounds check out, and if you add up the counts
for 0-5 and 9-95, they check out, too. But if you look more closely, you will
notice one value that has to be an error, a 51 pound baby! This has been cleaned in the cleanFemPreg function.

### Interpretation
To work with data effectively, you have to think on two levels at the same time: the level of statistics and the level of context.
As an example, let's look at the sequence of outcomes for a few respondents.
This example looks up one respondent and prints a list of outcomes for her
pregnancies:
```{r}
CASEID = 10229
femPregCleaned %>%
  filter(caseid==CASEID) %>%
  .$outcome
```
The outcome code 1 indicates a live birth. Code 4 indicates a miscarriage; that is, a pregnancy that ended spontaneously, usually with no known medical cause.

Statistically this respondent is not unusual. Miscarriages are common and there are other respondents who reported as many or more. But remembering the context, this data tells the story of a woman who was pregnant six times, each time ending in miscarriage. Her seventh and most recent pregnancy ended in a live birth. If we consider this data with empathy,
it is natural to be moved by the story it tells.

Each record in the NSFG dataset represents a person who provided honest answers to many personal and difficult questions. We can use this data to answer statistical questions about family life, reproduction, and health. At the same time, we have an obligation to consider the people represented by the data, and to afford them respect and gratitude.

## Distributions
```{r}
#helper theme for common visualizations
ditch_the_axes <- theme(
  axis.text = element_blank(),
  axis.line = element_blank(),
  axis.ticks = element_blank(),
  panel.border = element_blank(),
  panel.grid = element_blank(),
  axis.title = element_blank()
  )
```
### Histograms
One of the best ways to describe a variable is to report the values that appear in the dataset and how many times each value appears. This description is called the distribution of the variable.
The most common representation of a distribution is a histogram, which is a graph that shows the frequency of each value. In this context, "frequency" means the number of times the value appears.

When you start working with a new dataset, we suggest you explore the variables you are planning to use one at a time, and a good way to start is by looking at histograms.

We transformed agepreg from centiyears to years, and combined birthwgt_lb and birthwgt_oz into a single quantity, totalwgt_lb. In this section we use these variables to demonstrate some features of histograms.

We start by reading the data and selecting records for live births:
```{r}
live <- femPregCleaned %>%
  filter(outcome==1)
```
Next we generate and plot the histogram of birthwgt_lb for live births.
```{r}
live %>%
  ggplot(mapping = aes(birthwgt_lb)) + 
  geom_histogram(bins = 16, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "pounds", y="Frequency")
```
The most common value, called the mode, is 7 pounds. The distribution is approximately bell-shaped, which is the shape of the normal distribution, also called a Gaussian distribution. But unlike a true normal distribution, this distribution is asymmetric; it has a tail that extends farther to the left than to the right.

Let's look at the histogram for birthwgt_oz:
```{r}
live %>%
  ggplot(mapping = aes(birthwgt_oz)) + 
  geom_histogram(bins = 16, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "ounces", y="Frequency")
```
In theory we expect this distribution to be uniform; that is, all values should have the same frequency. In fact, 0 is more common than the other values, and 1 and 15 are less common, probably because respondents round off birth weights that are close to an integer value.

Let's look at the histogram for agepreg:
```{r}
live %>%
  ggplot(mapping = aes(agepreg)) + 
  geom_histogram(bins = 45, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "years", y="Frequency")
```
The mode is 21 years. The distribution is very roughly bell-shaped, but in this case the tail extends farther to the right than left; most mothers are in their 20s, fewer in their 30s.

Let's look at the prglength:
```{r}
live %>%
  ggplot(mapping = aes(prglngth)) + 
  geom_histogram(bins = 50, color = 'black', fill='light blue', na.rm = TRUE) +
  labs(x = "weeks", y="Frequency")
```
By far the most common value is 39 weeks. The left tail is longer than the right; early babies are common, but pregnancies seldom go past 43 weeks, and doctors often intervene if they do.

### Outliers
Looking at histograms, it is easy to identify the most common values and the shape of the distribution, but rare values are not always visible. Before going on, it is a good idea to check for outliers, which are extreme values that might be errors in measurement and recording, or might be accurate reports of rare events.

```{r}
live['prglngth'] %>%
  distinct %>% 
  top_n(-10) %>%
  arrange(prglngth)
```
In the list of pregnancy lengths for live births, the 10 lowest values are [0, 4, 9, 13, 17, 18, 19, 20, 21, 22]. Values below 10 weeks are certainly errors; the most likely explanation is that the outcome was not coded correctly. Values higher than 30 weeks are probably legitimate. Between 10 and 30 weeks, it is hard to be sure; some values are probably errors, but some represent premature babies.

On the other end of the range, the highest values are:
```{r}
live['prglngth'] %>%
  distinct %>% 
  top_n(7) %>%
  arrange(prglngth)
```
Most doctors recommend induced labor if a pregnancy exceeds 42 weeks, so some of the longer values are surprising. In particular, 50 weeks seems medically unlikely.

The best way to handle outliers depends on "domain knowledge"; that is, information about where the data come from and what they mean. And it depends on what analysis you are planning to perform.
In this example, the motivating question is whether first babies tend to be early (or late). When people ask this question, they are usually interested in full-term pregnancies, so for this analysis we will focus on pregnancies longer than 27 weeks.
### First babies
Now we can compare the distribution of pregnancy lengths for first babies and others. We divide the data of live births using birthord to create a new column birthNumber, and compute their histograms:
```{r}
live['birthNumber'] <- if_else(live$birthord==1, 'first', 'other')
```

```{r}
live %>%
  ggplot(aes(x=prglngth, fill=birthNumber)) + 
  geom_histogram(bins=20, position="dodge", na.rm = TRUE, color='black') +
  labs(x="Weeks", y="Frequency") + 
  theme(legend.position = c(0.2, 0.7)) + 
  xlim(27, 46)
```
Histograms are useful because they make the most frequent values immediately apparent. But they are not the best choice for comparing two distributions. In this example, there are fewer "first babies" than "others," so some of the apparent differences in the histograms are due to sample sizes. In the next chapter we address this problem using probability mass functions.

### Effect Size
An effect size is a summary statistic intended to describe the size of an effect. For example, to describe the difference between two groups, one obvious choice is the difference in the means.
Mean pregnancy length for first babies is 38.601; for other babies it is 38.523. The difference is 0.078 weeks, which works out to 13 hours. As a fraction of the typical pregnancy length, this difference is about 0.2%. 
If we assume this estimate is accurate, such a difference would have no practical consequences. In fact, without observing a large number of pregnancies, it is unlikely that anyone would notice this difference at all.
Another way to convey the size of the effect is to compare the difference between groups to the variability within groups. Cohen's d is a statistic intended to do that; it is defined
d = (_x1_ - _x2_)/s
where _x1_ and _x2_ are the means of the groups and s is the "pooled standard
deviation".
```{r}
CohenEffectSize <- function(group1, group2){
  diff = mean(group1, na.rm = TRUE) - mean(group2, na.rm = TRUE)
  
  var1 = var(group1, na.rm=TRUE)
  var2 = var(group2, na.rm=TRUE)
  n1 = length(group1)
  n2 = length(group2)
  
  pooled_var = (n1*var1 + n2*var2)/(n1+n2)
  d = diff/sqrt(pooled_var)
  return (d)
}
```

Let's look at the effect size for difference in totalwgt_lb for first babies vs other babies. 
```{r}
firstBirthWgt <- live %>%
  filter(birthord==1) %>%
  .$totalwgt_lb

othersBirthWgt <- live %>%
  filter(birthord!=1) %>%
  .$totalwgt_lb
CohenEffectSize(firstBirthWgt, othersBirthWgt)
```

## Probability Mass Functions
By plotting the PMF instead of the histogram, we can compare the two distributions without being mislead by the difference in sample size.
```{r}
live %>%
  ggplot(aes(x=prglngth, ..density.., fill=birthNumber)) + 
  geom_histogram(bins = 20, stat="bin", position = "dodge", na.rm = TRUE, color='black') +
  labs(x="weeks", y="probability") + 
  theme(legend.position = c(0.2, 0.7)) + 
  xlim(27, 46)
```
```{r}
live %>%
  ggplot(aes(x=prglngth, ..density.., color=birthNumber)) +
  geom_step(binwidth = 0.7, stat="bin", position = "identity", na.rm = TRUE) +
  labs(x="weeks", y="probability") + 
  theme(legend.position = c(0.2, 0.7)) + 
  xlim(27, 46)
```

Based on this figure, first babies seem to be less likely than others to arrive on time (week 39) and more likely to be a late (weeks 41 and 42).

### Other visualizations
Histograms and PMFs are useful while you are exploring data and trying to identify patterns and relationships. Once you have an idea what is going on, a good next step is to design a visualization that makes the patterns you have identified as clear as possible.

In the NSFG data, the biggest differences in the distributions are near the mode. So it makes sense to zoom in on that part of the graph, and to transform the data to emphasize differences:

```{r}
#get the proportion of different number of weeks for prglngth, by order of birth
prglngthProp <- (table(live$prglngth, live$birthNumber) %>%
  prop.table(2) * 100 )%>%
  data.frame()
prglngthProp['weeks'] <- prglngthProp[, 'Var1'] %>%
  as.character %>%
  as.integer
```

```{r}
#filter for zooming from 35 to 46 weeks
p1 <- prglngthProp %>%
  subset(weeks>=35 & weeks< 46 & Var2=='first') %>%
  .[c('weeks', 'Freq')]
  
p2 <- prglngthProp %>%
  subset(weeks>=35 & weeks< 46 & Var2=='other') %>%
  .[c('weeks', 'Freq')]
```

```{r}
#find difference between the frequencies
diff <- within(merge(p1, p2, by='weeks'), {
  weeks <- weeks
  percent_points <- Freq.x - Freq.y
})[c('weeks', 'percent_points')]
```


```{r}
#plot the differences as column graph
diff %>%
  ggplot(aes(weeks, percent_points)) +
  geom_col(color='black', fill='purple') +
  labs(x="weeks", y="percentage points")
```
This figure makes the pattern clearer: first babies are less likely to be born in week 39, and somewhat more likely to be born in weeks 41 and 42.

## Cumulative Distribution Functions
### Limitations of PMF
PMFs work well if the number of values is small. But as the number of values increases, the probability associated with each value gets smaller and the effect of random noise increases.

```{r}
live %>%
  ggplot(aes(x=totalwgt_lb, ..density.., fill=birthOrder)) + 
  geom_histogram(bins=100, stat="bin", na.rm = TRUE, color='black') +
  labs(x="weight(pounds)", y="probability") + 
  theme(legend.position = c(0.2, 0.7)) 
```

Overall, these distributions resemble the bell shape of a normal distribution, with many values near the mean and a few values much higher and lower. But parts of this figure are hard to interpret. There are many spikes and valleys, and some apparent differences between the distributions. It is hard to tell which of these features are meaningful. Also, it is hard to see overall patterns; for example, which distribution do you think has the higher mean?

An alternative that avoids these problems is the cumulative distribution function (CDF).

### CDF
```{r}
live %>%
  ggplot(aes(x=prglngth)) +
  stat_ecdf(geom="step", na.rm = TRUE, color='blue') +
  labs(x="weeks", y="CDF") + 
  theme(legend.position = c(0.2, 0.7)) 
```

One way to read a CDF is to look up percentiles. For example, it looks like about 10% of pregnancies are shorter than 36 weeks, and about 90% are shorter than 41 weeks. 
The CDF also provides a visual representation of the shape of the distribution. Common values appear as steep or vertical sections of the CDF; in this example, the mode at 39 weeks is apparent. There are few values below 30 weeks, so the CDF in this range is flat.

### Comparing CDFs
CDFs are especially useful for comparing distributions. For example, here is the code that plots the CDF of birth weight for first babies and others.
```{r}
live %>%
  ggplot(aes(x=totalwgt_lb, color=birthOrder)) +
  stat_ecdf(geom="step", na.rm = TRUE) +
  labs(x="weight(pounds)", y="CDF") + 
  theme(legend.position = c(0.2, 0.7)) 
```
Compared to PMFs, this figure makes the shape of the distributions, and the differences between them, much clearer. We can see that first babies are slightly lighter throughout the distribution, with a larger discrepancy above the mean.

### Percentile-based statistics
Once you have computed a CDF, it is easy to compute percentiles and percentile ranks. Percentile can be used to compute percentile-based summary statistics. For example, the 50th percentile is the value that divides the distribution in half, also known as the median.
```{r}
#50th percentile of pregnancy lengths
live$prglngth %>%
  quantile(.50, na.rm = TRUE)
#different quartiles of total birth weight
live$totalwgt_lb %>%
  summary
```

### Random Numbers
Suppose we choose a random sample from the population of live births and look up the percentile rank of their birth weights. Now suppose we compute the CDF of the percentile ranks. What do you think the distribution will look like?
```{r}
live[sample(nrow(live), 100, replace = TRUE),] %>% 
 mutate(percrank=rank(totalwgt_lb)/length(totalwgt_lb)) %>%
  ggplot(aes(x=percrank)) +
  stat_ecdf(geom="step", na.rm = TRUE, color='blue') +
  labs(x="precentile rank", y="CDF")
```
The CDF is approximately a straight line, which means that the distribution is uniform.

```{r}
live[sample(nrow(live), 100, replace = TRUE),] %>% 
 mutate(percrank=rank(totalwgt_lb)/length(totalwgt_lb)) %>%
  ggplot(aes(x=percrank, ..density..)) + 
  geom_histogram(stat="bin", na.rm = TRUE, color='black', fill='light blue') +
  labs(x="percentile rank", y="probability")
```

That outcome might be non-obvious, but it is a consequence of the way the CDF is defined. What this figure shows is that 10% of the sample is below the 10th percentile, 20% is below the 20th percentile, and so on, exactly as we should expect. So, regardless of the shape of the CDF, the distribution of percentile ranks is uniform.

## Comparing percentile ranks
Percentile ranks are useful for comparing measurements across different groups. For example, people who compete in foot races are usually grouped by age and gender. To compare people in different age groups, you can convert race times to percentile ranks.

## Modeling Distributions
The distributions we have used so far are called empirical distributions because they are based on empirical observations, which are necessarily finite samples.
The alternative is an analytic distribution, which is characterized by a CDF that is a mathematical function. Analytic distributions can be used to model empirical distributions. In this context, a model is a simplification that leaves out unneeded details. This chapter presents common analytic distributions and uses them to model data from a variety of sources.

### Exponential Distribution
The CDF of the exponential distribution is: CDF(x) = 1 - e^(-lambda*x). The parameter, lambda, determines the shape of the distribution.
```{r}
data.frame(x = c(0,3.0)) %>%
  ggplot(mapping = aes(x)) +
  stat_function(fun=pexp, args=list(rate=0.5), aes(colour='0.5')) + 
  stat_function(fun=pexp, args=list(rate=1), aes(colour='1')) + 
  stat_function(fun=pexp, args=list(rate=2), aes(colour='2')) + 
  scale_color_manual('lambda', values = c('green', 'light blue', 'blue')) +
  labs(x="x", y="CDF")
```
This figure shows how the CDF of exponential distribution for lambda = 0.5, 1, and 2 looks like.

In the real world, exponential distributions come up when we look at a series of events and measure the times between events, called interarrival times. If the events are equally likely to occur at any time, the distribution of interarrival times tends to look like an exponential distribution.

As an example, we will look at the interarrival time of births. On December 18, 1997, 44 babies were born in a hospital in Brisbane, Australia. The time of birth for all 44 babies was reported in the local paper; the complete dataset is in a file called babyboom.dat, in the ThinkStats2 repository. It has 4 columns time, sex, weight_g, and minutes, where minutes is time of birth converted to minutes since midnight.
```{r}
babyBoom <- read.fwf('~/Documents/CodeWork/ThinkStats2/code/babyboom.dat',skip = 59, widths = c(8, 8, 8, 8), col.names = c('time', 'sex', 'weight_g', 'minutes'))
```

```{r}
bab
```

